<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Breaking Barriers, Localizing Saliency: A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description" content="Breaking Barriers, Localizing Saliency: A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection.">
  <meta name="keywords" content="CSOD">
  <link rel="author" href="https://rmcong.github.io">
  <!--=================js==========================-->
  <link href="./project/DAFNet_files/css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project/DAFNet_files/project.css" media="screen">
  <script type="text/javascript" async="" src="./project/DAFNet_files/analytics.js.download"></script>
  <script src="./project/DAFNet_files/effect.js.download"></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async="" src="./project/DAFNet_files/latest.js.download">
    </script>
  <!--=================Google Analytics==========================-->
  <script async="" src="./project/DAFNet_files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
<script type="text/javascript" async="" src="./project/DAFNet_files/MathJax.js.download"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

<body><div id="MathJax_Message" style="display: none;"></div>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
        Breaking Barriers, Localizing Saliency:<br> A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection
        </h1>
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://rmcong.github.io/" target="_blank">Runmin Cong</a> <sup>1</sup>&nbsp;
          Zhiyang Chen <sup>1</sup> &nbsp;
          <a href="https://fanghaook.github.io/" target="_blank">Hao Fang</a> <sup>1</sup> &nbsp;
          <a href="https://www.ln.edu.hk/po/people/professor-sam-kwong-tak-wu" target="_blank">Sam Kwong</a> <sup>2</sup> &nbsp;
          <a href="http://www.vsislab.com" target="_blank">Wei Zhang</a> <sup>1</sup> &nbsp;
          
        </div>

        <div class="affiliations ">
          <sup>1</sup> Shandong University, Jinan, China<br>
          <sup>2</sup> Lingnan University, Hong Kong SAR, China<br>
          
          
          
        </div>
        <!--=================Tabs==========================
        <ul id="tabs">
          <li><a href="https://li-chongyi.github.io/Zero.html#materials" name="#tab1">Materials</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#pcd_tsa" name="#tab2">PCD &amp; TSA</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#ablations" name="#tab3">Ablations</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#results" name="#tab4">Results</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#citation" name="#tab5">Citation</a></li>
      </ul>-->
      </div>
      <br>
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p>
          <div style="text-align: justify; display: block; margin-right: auto;">
          <p>Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, \etc, which we call constrained conditions. 
          These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition.
          To bridge this gap, we launch a new task, <b> Condition-Constrained Salient Object Detection (CSOD)</b>, aimed at robustly and accurately locating salient objects in constrained environments.
          On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. 
          This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging.
          On the other hand, we abandon the paradigm of "restoration before detection" and instead introduce a unified <b>end-to-end framework CSSAM </b>that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead.
          Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes.
          To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task.
          Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.
       </p>
        </p>
      </div>
      <br/> 

      <!--=================Teasers==========================-->
      <div id="examples" class="img_container">
          <h2>CSOD10K Dataset</h2>
      <br>
      <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/CSOD_files/example.png">
            </div>
          </center>
        </div>
           <div class="section">
        <div style="text-align: justify; display: block; margin-right: auto;">
        <p>This study introduces the CSOD10K dataset, a new large-scale benchmark explicitly designed for salient object detection in various constrained conditions, 
          which enables us to train and evaluate state-of-the-art SOD methods. The above figure presents some images from the CSOD10K dataset along with their ground truth.</p>
      </div>
        </div>
              <br/> 
              <br/> 
      <div id="img_intro_examples" class="img_container">
      <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/CSOD_files/dataset.png">
            </div>
          </center>
           <div class="section">
        <div style="text-align: justify; display: block; margin-right: auto;">
        <p>Statistics from the CSOD10K dataset. (a) Fine-grained classification of constrained scenes, where the bar length represents the number of images. 
          (b) Number of images for each salient object category. (c) Image resolution distribution. (d) Distribution of the number of salient objects per image. 
          (e) The comparison of the size of salient objects with other SOD datasets, where the horizontal axis refers to the proportion of salient objects mask to the total number of pixels in the image. 
          (f) Comparison of CSOD10K and other datasets in color contrast.</p>
      </div>
        </div>
        <br/>  
      <!--=================Pipeline==========================-->
      <div id="img_intro_examples" class="img_container">
          <h2>Pipeline</h2>
      <br>
               <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/CSOD_files/main.png">
            </div>
          </center>
            <br>
        <p>
           <div class="section">
          <div style="text-align: justify; display: block; margin-right: auto;">
          <p>The overall framework of the proposed CSSAM. 
            During the encoding stage, we freeze the parameters of the image encoder and incorporate the Scene Prior-Guided Adapter (SPGA) into the encoder for parameter-efficient fine-tuning to adapt to constrained scenes. 
            In the decoding stage, we use Hybrid Prompt Decoding Strategy (HPDS) to adapt to the SOD task. First, we obtain four types of prompts through the Positive and Negative Prompt Generator. 
            Second, the Prompt Integration Module (PIM) fuses multiple prompts. Finally, the mask decoder outputs the saliency mask.
        </p></div>
      <br/>
        </div>
      <!--=================Highlights==========================-->
     
      <div id="img_intro_examples" class="img_container">
        <h2>Highlights</h2>
        <br>
        <div style="text-align: justify; display: block; margin-right: auto;">
        <ol>
          <li><p>We launch a new task, <b>Condition-Constrained Salient Object Detection (CSOD)</b>, and provide solutions from the two dimensions of data and model, enabling intelligent systems to more reliably address complex visual challenges in real and open environments.</p></li>
          <li><p>We construct a large-scale benchmark dataset <b>CSOD10K</b> for the CSOD task. It is the first SOD dataset covering diverse constrained conditions, with <b>10,000 images</b>, 3 main constraint types, 8 real-world scenes, 101 object categories, and pixel-level annotations, providing a rich experimental basis and exploration opportunities for the research community.</p></li>
          <li><p>We propose <b>a unified end-to-end framework CSSAM</b> for the CSOD task. We design a <b>Scene Prior-Guided Adapter (SPGA)</b> to enable the foundation model to better adapt to downstream constrained scenes. We propose a <b>Hybrid Prompt Decoding Strategy (HPDS)</b> that effectively generates and integrates multiple types of prompts to achieve adaptation to the SOD task.</p></li>
        </ol>
      </div>
      <br/>
      <!--=================Applications==========================<div class="section" ,="" id="results">-->
        <div id="img_intro_examples" class="img_container">
        <h2>Results</h2>
        <br>
        <!--=================*******==========================-->
        <div id="vid4" class="img_container">
          <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/CSOD_files/compare.png">
            </div>
          </center>
        </div>
        <br/>
        
            <!--=================Materials==========================<div class="section materials" ,="" id="materials">-->
      <div class="img_container" ,="" id="materials">
        <h2>Materials</h2>
        <br>
        <table width="100%" align="center" border="none" cellspacing="0" cellpadding="30">
          <tbody><tr>
            <!-- <td width="20%">
              <center>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434" target="_blank" class="imageLink"><img src="./project/txt.png" ,="" width="40%"></a><br><br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434" target="_blank">Paper</a>
              </center>
            </td> -->
            <td width="20%">
              <center>
                <a href="https://github.com/ettof/CSOD" target="_blank" class="imageLink"><img src="./project/icon_github.png" ,="" width="40%"></a><br><br>
                <a href="https://github.com/ettof/CSOD" target="_blank">CSOD Dataset</a>
              </center>
            </td>
            <td width="20%">
              <center>
                <a href="https://github.com/ettof/CSOD" target="_blank" class="imageLink"><img src="./project/icon_github.png" ,="" width="40%"></a><br><br>
                <a href="https://github.com/ettof/CSOD" target="_blank">Code and Model</a>
              </center>
            </td>
            <td width="20%">
              <center>
                <a href="https://mp.weixin.qq.com/s/-Lkbgrn1s1AaYP12U3qBUg" target="_blank" class="imageLink"><img src="./project/link.png" ,="" width="40%"></a><br><br>
                <a href="https://mp.weixin.qq.com/s/-Lkbgrn1s1AaYP12U3qBUg" target="_blank">中文导读</a>
              </center>
            </td> 
            <td width="20%">
              <center>
                <a href="https://www.bilibili.com/video/BV1merjBbEAy/?vd_source=6432d91082e1f71a4035ed4c6a875caa" target="_blank" class="imageLink"><img src="./project/talk.png" ,="" width="40%"></a><br><br>
                <a href="https://www.bilibili.com/video/BV1merjBbEAy/?vd_source=6432d91082e1f71a4035ed4c6a875caa" target="_blank">中文讲解视频</a>
              </center>
            </td>
          </tr>
        </tbody></table>
      </div>
          <br/>
      <!--=================Citation==========================<div class="section citation" ,="" id="citation">-->
      <!-- <div class="img_container" ,="" id="citation">
        <h2>Citation</h2>
        <div class="section bibtex">
          <pre>@article{DAFNet,
  title={Dense attention fluid network for salient object detection in optical remote sensing images},
  author={Zhang, Qijian and Cong, Runmin and Li, Chongyi and Cheng, Ming-Ming and Fang, Yuming and Cao, Xiaochun and Zhao, Yao and Kwong, Sam},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={1305-1317},
  year={2021},
  publisher={IEEE}
}
          </pre>
        </div>
      </div>
          <br/> -->
      <!--=================Contact==========================<div class="section contact">-->
      <div id="img_intro_examples" class="img_container">
        <h2 id="contact">Contact</h2>
        <br>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you have any questions, please contact Runmin Cong at <strong>rmcong@sdu.edu.cn</strong>.</p> 
      </div>


</div></div></body></html>

