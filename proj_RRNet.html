<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description" content="RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images.">
  <meta name="keywords" content="RRNet">
  <link rel="author" href="https://rmcong.github.io">
  <!--=================js==========================-->
  <link href="./project/css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project/project.css" media="screen">
  <script type="text/javascript" async="" src="./project/analytics.js.download"></script>
  <script src="./project/effect.js.download"></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async="" src="./project/latest.js.download">
    </script>
  <!--=================Google Analytics==========================-->
  <script async="" src="./project/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
<script type="text/javascript" async="" src="./project/MathJax.js.download"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

<body><div id="MathJax_Message" style="display: none;"></div>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
        RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images
        </h1>
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://rmcong.github.io/" target="_blank">Runmin Cong</a> <sup>1</sup>&nbsp;
          Yumo Zhang<sup>1</sup>&nbsp;
          <a href="https://sites.google.com/site/leyuanfang/home" target="_blank">Leyuan Fang</a> <sup>2</sup> &nbsp;
          <a href="http://gp.sysu.edu.cn/zh-hans/teacher/173" target="_blank">Jun Li</a> <sup>3</sup> &nbsp;
          <a href="http://faculty.bjtu.edu.cn/5900/" target="_blank">Yao Zhao</a> <sup>1</sup> &nbsp;
          <a href="https://www.cs.cityu.edu.hk/~cssamk/research_group/" target="_blank">Sam Kwong</a> <sup>4</sup> &nbsp;
          
        </div>

        <div class="affiliations ">
          <sup>1</sup> Beijing Jiaotong University, Beijing, China<br>
          <sup>2</sup> Hunan University, Changsha, China<br>
          <sup>3</sup> Sun Yat-sen University, Guangzhou, China<br>
          <sup>4</sup> City University of Hong Kong, China<br>

          
          
          
        </div>
        <!--=================Tabs==========================
        <ul id="tabs">
          <li><a href="https://li-chongyi.github.io/Zero.html#materials" name="#tab1">Materials</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#pcd_tsa" name="#tab2">PCD &amp; TSA</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#ablations" name="#tab3">Ablations</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#results" name="#tab4">Results</a></li>
          <li><a href="https://li-chongyi.github.io/Zero.html#citation" name="#tab5">Citation</a></li>
      </ul>-->
      </div>
      <br>
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p>
          <div style="text-align: justify; display: block; margin-right: auto;">
          <p>Salient object detection (SOD) for optical remote sensing images (RSIs) aims at locating and extracting visually distinctive objects/regions from the optical RSIs.
            Despite some saliency models were proposed to solve the intrinsic problem of optical RSIs (such as complex background and scale-variant objects), 
            the accuracy and completeness are still unsatisfactory. To this end, <b>we propose a relational reasoning network with parallel multi-scale attention for SOD
            in optical RSIs</b> in this paper. The relational reasoning module that integrates the spatial and the channel dimensions is designed to infer the semantic relationship
            by utilizing high-level encoder features, thereby promoting the generation of more complete detection results. The parallel multi-scale attention module is proposed
            to effectively restore the detail information and address the scale variation of salient objects by using the low-level features refined by multi-scale attention.
            Extensive experiments on two datasets demonstrate that our proposed RRNet outperforms the existing state-of-the-art SOD competitors both qualitatively and quantitatively. 
            </p>
        </p>
      </div>
      <br/>
      <!--=================Teasers==========================-->
      <div id="img_intro_examples" class="img_container">
          <h2>Pipeline</h2>
      <br>
      <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/RRNet_files/framework.jpg">
            </div>
          </center>
        </div>
           <div class="section">
        <div style="text-align: justify; display: block; margin-right: auto;">
        <p><b>Architecture of RRNet</b>, consisting of a relational reasoning encoder and a multi-scale attention decoder. 
          The encoder generates hierarchical features, i.e., low-level features from the first two stages and high-level features from the last three stages.
          Relational reasoning in two dimensions following each high-level stage are successively employed to refine features by reasoning semantic relationship.
          Low-level features obtained by encoder are fed into parallel multi-scale attention module, generating attention maps with valuable information to restore lost details.
          The top right portion is the computation procedures of feature fusion between passed-up deep features and shallow features.</p>
      </div>
        </div>
        <br/> 
  

      
      <!--=================Highlights==========================-->
     
      <div id="img_intro_examples" class="img_container">
        <h2>Highlights</h2>
        <br>
        <div style="text-align: justify; display: block; margin-right: auto;">
        <ol>
          <li><p>We propose <b>a novel end-to-end relational reasoning network with parallel multi-scale attention (RRNet)</b> for SOD in optical RSIs, which consists of a relational reasoning encoder and a multi-scale attention decoder.</p></li>
          <li><p>We design a <b>relational reasoning module in the high-level layers of the encoder stage</b> to model the sematic relations and force the generation of complete salient objects. <b>This is the first attempt to introduce relational reasoning in the SOD framework for optical RSIs.</b> Moreover, we innovatively employ relational reasoning along the spatial and channel dimensions jointly to obtain more comprehensive semantic relations.</p></li>
          <li><p>We propose a <b>parallel multi-scale attention scheme in the low-level layers of the decoder stage</b> to recover the detail information in a multi-scale and attention manner. This mechanism can deal with the object scale variation issue through the multi-scale design, while effectively recovering the detail information with the help of shallower features selected by the parallel attention.</p></li>
          <li><p>We compare the proposed methods with thirteen state-of-the-art approaches on two challenging optical RSI datasets. Without bells and whistles, our method achieves the best performance under three evaluation metrics. Besides, the model has a real-time inference speed of <b>109 FPS</b>.</p></li>
        </ol>
      </div>
      <br/>
        
        
        <!--=================Teasers==========================-->
      <div id="img_intro_examples" class="img_container">
          <h2>Qualitative Evaluation</h2>
      <br>
       <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/RRNet_files/visual.jpg">
            </div>
          </center>
        </div>
           <div class="section">
        <div style="text-align: justify; display: block; margin-right: auto;">
        <p><b>Visual comparisons of our proposed method and SOTA methods on EORSSD dataset</b>. The deep learning based methods are trained/re-trained on the EORSSD dataset.</p>
      </div>
        </div>
        <br/> 

         <!--<ol>
        <center>
          <div class="leftView">
            <div class="mask" style="width:80px;height:80px"></div>
            <img class="std" src="./project/DPANet_files/visual.png">
          </div>
        </center>
         </ol>
      </div>
      <div class="section">
        <div style="text-align: justify; display: block; margin-right: auto;">
       <ol>
        <p>Qualitative comparison of the proposed approach with some state-of-the-art RGB and RGB-D SOD methods, in which our results are highlighted by a red box. 
          (a) RGB image. (b) Depth map. (c) GT. (d) DPANet. (e) PiCAR. (f) PoolNet. (g) BASNet. (h) EGNet. (i) CPFP. (j) PDNet. (k) DMRA. (l) AF-Net.</p>
       </ol>
      </div>
        </div>
      <br/>
      
      <!--=================Applications==========================<div class="section" ,="" id="results">-->
        <div id="img_intro_examples" class="img_container">
        <h2>Quantitative Evaluation</h2>
        <br>
        <!--=================*******==========================-->
        <div id="vid4" class="img_container">
          <center>
            <div class="leftView">
              <div class="mask" style="width:80px;height:80px"></div>
              <img class="std" src="./project/RRNet_files/performance.png">
            </div>
          </center>
        </div>
        <br/>
        
            <!--=================Materials==========================<div class="section materials" ,="" id="materials">-->
      <div class="img_container" ,="" id="materials">
        <h2>Materials</h2>
        <br>
        <table width="100%" align="center" border="none" cellspacing="0" cellpadding="30">
          <tbody><tr>
            <td width="20%">
              <center>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592773" target="_blank" class="imageLink"><img src="./project/txt.png" ,="" width="40%"></a><br><br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592773" target="_blank">Paper</a>
              </center>
            </td>
            <td width="20%">
              <center>
                <a href="https://github.com/rmcong/RRNet_TGRS2021" target="_blank" class="imageLink"><img src="./project/icon_github.png" ,="" width="40%"></a><br><br>
                <a href="https://github.com/rmcong/RRNet_TGRS2021" target="_blank">Code and Model</a>
              </center>
            </td>
          </tr>
        </tbody></table>
      </div>
          <br/>
          
      <!--=================Citation==========================<div class="section citation" ,="" id="citation">-->
      <div class="img_container" ,="" id="citation">
        <h2>Other works</h2>
        <br>
        <div style="text-align: justify; display: block; margin-right: auto;">
        <ol>
          <li><p>Qijian Zhang, Runmin Cong, Chongyi Li, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, and Sam Kwong, Dense attention fluid network for salient object detection in optical remote sensing images, IEEE Transactions on Image Processing, vol. 30, pp. 1305-1317, 2021. <a href="https://rmcong.github.io/proj_DPANet.html"  target="_blank"><font color="#FF5151">[Project Page]</font></a></p></li>
          <li><p>Chongyi Li, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, and Sam Kwong, Nested network with two-stream pyramid for salient object detection in optical remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 11, pp. 9156-9166, 2019. <a href="https://li-chongyi.github.io/proj_optical_saliency.html"  target="_blank"><font color="#FF5151">[Project Page]</font></a></p></li>
        <li><p>Chongyi Li, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, A parallel down-up fusion network for salient object detection in optical remote sensing images, Neurocomputing, vol. 415, pp. 411-420, 2020.</p></li>
          </ol>
      </div>
      <br/>
          
          
      <!--=================Citation==========================<div class="section citation" ,="" id="citation">-->
      <div class="img_container" ,="" id="citation">
        <h2>Citation</h2>
        <div class="section bibtex">
          <pre>@article{RRNet,
  title={{RRNet}: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images},
  author={Cong, Runmin and Zhang, Yumo and Fang, Leyuan and Li, Jun and Zhao, Yao and Kwong, Sam},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={60},
	pages={1558-1644},
  year={2022},
  publisher={IEEE}
}
         </pre>
        </div>
      </div>
          <br/>
      <!--=================Contact==========================<div class="section contact">-->
      <div id="img_intro_examples" class="img_container">
        <h2 id="contact">Contact</h2>
        <br>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you have any questions, please contact Runmin Cong at <strong>rmcong@bjtu.edu.cn</strong>.</p> 
      </div>


</div></div></body></html>
