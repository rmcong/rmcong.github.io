<!DOCTYPE html>
<!-- saved from url=(0057)http://www.icst.pku.edu.cn/struct/people/yangs/index.html -->
<html class="csstransforms csstransforms3d csstransitions"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		
		<title>Runmin Cong</title>
		<link href="./files/bootstrap.css" rel="stylesheet" type="text/css" media="all">
		<link href="./files/style.css" rel="stylesheet" type="text/css" media="all">
		<link href="./files/prettyPhoto.css" rel="stylesheet" type="text/css" media="all">		
		<link href="./files/css" rel="stylesheet" type="text/css">	
	</head>
	<body>
		<!---start-wrap--->
		<!---start-header--->
	<div class="header">
		<div class="wrap">
			<!---start-logo--->
			<div class="logo">
				<a href="./index.html">Runmin Cong</a>
			</div>
			<!---End-logo--->
			<!---start-top-nav--->
			<div class="top-nav">
				<ul>
					<li id="Home" onclick="func(&#39;Me&#39;)"><a href="./index.html" class="scroll">Home</a></li>
					<li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="./Profile.html" class="scroll">Profile</a></li>
					<li id="Publications" onclick="func(&#39;Publications&#39;)"><a href="./Publication.html" class="scroll">Publications</a></li>
					<!--<li id="Profile-chinese" onclick="func(&#39;Honors Awards&#39;)"><a href="./Honors Awards.html" class="scroll">Honors & Awards</a></li>-->
					<li id="MVPLab" onclick="func(&#39;MVP Lab&#39;)"><a href="./MVPLab.html" class="scroll">MVP Lab</a></li>
                                        <!--<li id="Projects" onclick="func(&#39;Projects&#39;)"><a href="./Project.html" class="scroll">Projects</a></li>-->
					<!-- <li id="Art" onclick="func(&#39;Art&#39;)"><a href="http://www.icst.pku.edu.cn/struct/people/yangs/index.html#art" class="scroll">Art Gallery</a></li>	-->
					<li id="Profile-chinese" onclick="func(&#39;Profile-chinese&#39;)"><a href="./Profile-chinese.html" class="scroll">中文简介</a></li>
					<li id="Contact" onclick="func(&#39;Contact&#39;)"><a href="./Contact.html" class="scroll">Contact</a></li>
				</ul>
			</div>
			<div class="clear"> </div>
			<!---End-top-nav--->
	     </div>
	</div>
	
	<script> 
	var lastname = 'Me';
	function func(name){
		var div = document.getElementById(name); 
		div.className = 'active'; 
		div = document.getElementById(lastname); 
		div.className = ''; 
		lastname = name;
	}
	function coming_soon()
	{
		alert("Coming soon.");
	}
	</script> 
	
		<!---End-wrap--->
	<div class="content">
		<div class="grid3">
			<div class="grid3-content">
		<h3><b>Selected Publications and Patents:</b></h3>
	
                

	
	<!--COPYRIGHT: The copyright of the following materials belongs to corresponding publishers. 
They are provided only for research and educational use that does not conflict to the interests of the publishers.
-->
	  </ul>
  <hr />			
<!-- 			
<h4><b>Pre-print:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
				

  </ul>
  <hr />-->
				

<h4><b>Books:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">

<li > <strong>Runmin Cong</strong>, Hao Chen, Hongyuan Zhu, and Huazhu Fu, 
  <a href="https://link.springer.com/chapter/10.1007/978-3-030-28603-3_10" target="_blank"><font color="#0000FF">Foreground detection and segmentation in RGB-D images</font></a>,
  in <ud2><i>Paul Rosin, Yukun Lai, Yonghuai Liu, and Ling Shao, RGB-D Image Analysis and Processing</i>, Springer</ud2>, ISBN 978-3-030-28602-6, Dec. 2019. (Book Chapter) <br></li>

<li > Chongyi Li, Huazhu Fu, Miao Yang, <strong>Runmin Cong</strong>, and Chunle Guo, 
  <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811218842_0010" target="_blank"><font color="#0000FF">Deep retinal image non-uniform illumination removal</font></a>,
  in <ud2><i>Zhenghua Chen, Min Wu, and Xiaoli Li, Generalization with Deep Learning: For Improvement on Sensing Capability</i>, World Scientific</ud2>, ISBN 978-981-121-883-5, Apr. 2021. (Book Chapter) <br></li>
  	
</ul>
	<hr />	
	
	
<h4><b>2023:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">	
	  
<li> <strong>Runmin Cong</strong>, Ning Yang, Chongyi Li, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9837785" target="_blank"><font color="#0000FF">Global-and-local collaborative learning for co-salient object detection</font></a>, 
  <ud2>IEEE Transactions on Cybernetics</ud2>, 2023. In Press.  <br>
  <a href="https://rmcong.github.io/proj_GLNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/GLNet_TCYB2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
<a href="https://www.bilibili.com/video/BV1FP411M74q/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>

<li >  <strong>Runmin Cong</strong>, Ke Huang, Jianjun Lei, Yao Zhao, Qingming Huang, and Sam Kwong, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Multi-projection fusion and refinement network for salient object detection in 360° omnidirectional image</font></a>,
  <ud2>IEEE Transactions on Neural Networks and Learning Systems</ud2>, 2023. In Press.  <br>
  <a href="https://rmcong.github.io/proj_MPFRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/MPFRNet_TNNLS2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
</li> 	 	  
	  
	  
<li> <strong>Runmin Cong</strong>, Kepu Zhang, Chen Zhang, Feng Zheng, Yao Zhao, Qingming Huang, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9926193" target="_blank"><font color="#0000FF">Does Thermal really always matter for RGB-T salient object detection?</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, 2023. In Press.  <br>
  <a href="https://rmcong.github.io/proj_TNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/TNet_TMM2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
 <a href="https://www.bilibili.com/video/BV1Jg411q7s9/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>
	  
<li> <strong>Runmin Cong</strong>, Qi Qin, Chen Zhang, Qiuping Jiang, Shiqi Wang, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9881551" target="_blank"><font color="#0000FF">A weakly supervised learning framework for salient object detection via hybrid labels</font></a>, 
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, 2023. In Press.  <br>
  <a href="https://rmcong.github.io/proj_Hybrid-Label-SOD.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Hybrid-Label-SOD_TCSVT2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>	  
	  
<li> <strong>Runmin Cong</strong>, Weiyu Song, Jianjun Lei, Guanghui Yue, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9955382" target="_blank"><font color="#0000FF">PSNet: Parallel symmetric network for video salient object detection</font></a>, 
  <ud2>IEEE Transactions on Emerging Topics in Computational Intelligence</ud2>, 2022. In Press.  <br>
  <a href="https://rmcong.github.io/proj_PSNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/PSNet_TETCI2022"  target="_blank"><font color="#FF5151">[Results]</font></a>  </li>
	  
<li> <strong>丛润民</strong>, 张晨, 徐迈, 刘鸿羽, 赵耀,
  <a href="http://www.jos.org.cn/jos/article/abstract/6700" target="_blank"><font color="#0000FF">深度学习时代下的RGB-D显著性目标检测研究进展</font></a>, 
  <ud2>软件学报</ud2>, 2023.  (CCF A) <br></li>	 

<li > Xiaofei Zhou, Kunye Shen, Li Weng, <strong>Runmin Cong*</strong>, Bolun Zheng, Jiyong Zhang, and Chenggang Yan,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9756846" target="_blank"><font color="#0000FF">Edge-guided recurrent positioning network for salient object detection in optical remote sensing images</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 53, no. 1, pp. 539-552, 2023. (* corresponding author)
  <a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <!--<font color="#999999">[Code]</font>-->
</li> 		  
	  
	  
<!---<li> Chunjie Zhang, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Structure decomposition of visual-semantic correlations for image classification with varied levels of supervision</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, 2023. In Press. (CCF A)<br>
  <a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>
</li> --->

<li > Yixuan Wu, Feng Li, <strong>Runmin Cong</strong>, Huihui Bai, Weisi Lin, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9721549" target="_blank"><font color="#0000FF">Bridging component learning with degradation modelling for blind image super-resolution</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, 2023. In Press. <strong><font color="#00CC33"><i>Popular Documents in TMM (2022/10)</i></font></strong>
  <a href="https://github.com/Arcananana/CDCN"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	  
	  
<li > Feng Li, Yixuan Wu, Huihui Bai, Weisi Lin, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9721549" target="_blank"><font color="#0000FF">Learning detail-structure alternative optimization for blind super-resolution</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, 2023. In Press. 
  <a href="https://github.com/Arcananana/DSSR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	  
	  
<li > Xiaoyu Zhang, Wei Gao, Ge Li, Qiuping Jiang, and <strong>Runmin Cong</strong>, 
  <a href="https://dl.acm.org/doi/pdf/10.1145/3532625" target="_blank"><font color="#0000FF">Image quality assessment driven reinforcement learning for mixed distorted image restoration</font></a>,
  <ud2>ACM Transactions on Multimedia Computing Communications and Applications</ud2>, 2023. In Press. <br>
</li> 
	  
 <li > Heyu Huang, <strong>Runmin Cong*</strong>, Lianhe Yang, Ling Du, Cong Wang, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3571744" target="_blank"><font color="#0000FF">Feedback chain network for hippocampus segmentation</font></a>,
  <ud2>ACM Transactions on Multimedia Computing Communications and Applications</ud2>, 2023. In Press. (* corresponding author)
  <!--<a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 
	  
<li> Jie Wu^, <strong>Runmin Cong</strong>^, Leyuan Fang, Chunle Guo, Bob Zhang, Pedram Ghamisi,
  <a href="http://scis.scichina.com/en/2023/119105.pdf" target="_blank"><font color="#0000FF">Unpaired remote sensing image super resolution with content-preserving weak supervision neural network</font></a>,
  <ud2>Science China Information Science</ud2>, vol. 66, no. 1, pp. 119105:1-119105:2, 2023. (CCF A, ^ equal contribution)<br>
  <!---<a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font> --->
</li>	
	  
	  </ul>
  <hr />
			
			
			
			
	
<h4><b>2022:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
<li> <strong>Runmin Cong</strong>, Qinwei Lin, Chen Zhang, Chongyi Li, Xiaochun Cao, Qingming Huang, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9930882" target="_blank"><font color="#0000FF">CIR-Net: Cross-modality interaction and refinement for RGB-D salient object detection</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 31, pp. 6800-6815, 2022.  <br>
  <a href="https://rmcong.github.io/proj_CIRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/CIRNet_TIP2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
 <a href="https://www.bilibili.com/video/BV1n24y117xW/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>

	  
	  
<li> <strong>Runmin Cong</strong>, Yumo Zhang, Leyuan Fang, Jun Li, Yao Zhao, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592773" target="_blank"><font color="#0000FF">RRNet: Relational reasoning network with parallel multi-scale attention for salient object detection in optical remote sensing images</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 60, pp. 1-11, 2022.  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_RRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/RRNet_TGRS2021"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>	
  <a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>


	  
<li> <strong>Runmin Cong</strong>, Haowei Yang, Qiuping Jiang, Wei Gao, Haisheng Li, Cong Wang, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9849697" target="_blank"><font color="#0000FF">BCS-Net: Boundary, context and semantic for automatic COVID-19 lung infection segmentation from CT images</font></a>, 
  <ud2>IEEE Transactions on Instrumentation and Measurement</ud2>, vol. 71, pp. 1-11, 2022. <br>
  <a href="https://github.com/rmcong/BCS-Net-TIM22"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>	 
	  
<li> <strong>Runmin Cong</strong>, Yumo Zhang, Ning Yang, Haisheng Li, Xueqi Zhang, Ruochen Li, Zewen Chen, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9882382" target="_blank"><font color="#0000FF">Boundary guided semantic learning for real-time COVID-19 lung infection segmentation system</font></a>, 
  <ud2>IEEE Transactions on Consumer Electronics</ud2>, vol. 68, no. 4, pp. 376-386, 2022. <strong><font color="#00CC33"><i>Popular Documents in TCE (2022/10)</i></font></strong><br>
  <a href="https://github.com/rmcong/BSNet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>
	  
	  
<li > Qiuping Jiang, Yudong Mao, <strong>Runmin Cong</strong>, Wenqi Ren, Chao Huang, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9757816" target="_blank"><font color="#0000FF">Unsupervised decomposition and correction network for low-light image enhancement</font></a>,
  <ud2>IEEE Transactions on Intelligent Transportation Systems</ud2>,  vol. 23, no. 10,  pp. 19440-19455, 2022.<br>
  <a href="https://github.com/myd945/UDCN"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li> 	

 
	  
 <li > Feng Li, Jia Qin, Huihui Bai, Weisi Lin, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/document/9827481" target="_blank"><font color="#0000FF">SRInpaintor: When super-resolution meets Transformer for image inpainting</font></a>,
  <ud2>IEEE Transactions on Computational Imaging</ud2>, vol. 8, pp. 743-758, 2022. <strong><font color="#00CC33"><i>Popular Documents in TCI (2022/08-2022/10)</i></font></strong><br>
  <!--<a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 	
	  
<li > Qiuping Jiang, Yuese Gu, Chongyi Li, <strong>Runmin Cong</strong>, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9749233" target="_blank"><font color="#0000FF">Underwater image enhancement quality evaluation: Benchmark dataset and objective metric</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 32, no. 9, pp. 5959-5974, 2022. <strong><font color="#00CC33"><i>Popular Documents in TCSVT (2022/04; 2022/09; 2022/10)</i></font></strong><br>
  <a href="https://github.com/yia-yuese/SAUD-Dataset"  target="_blank"><font color="#FF5151">[----SAUD-Dataset & NUIQ-Metric----]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 
	  
<li > Yudong Mao, Qiuping Jiang, <strong>Runmin Cong</strong>, Wei Gao, Feng Shao, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9435962" target="_blank"><font color="#0000FF">Cross-modality fusion and progressive integration network for saliency prediction on stereoscopic 3D images</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 24, pp. 2435-2448, 2022. <br>
</li>
	  
<li > Chun-Le Guo, Qixin Yan, Saeed Anwar, <strong>Runmin Cong</strong>, Wenqi Ren, and Chongyi Li, 
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf" target="_blank"><font color="#0000FF">Image dehazing transformer with transmission-aware 3D position embedding</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 5812-5820, 2022. (CCF A) <br>
<a href="https://li-chongyi.github.io/Proj_DeHamer.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
</li> 	
	  
<li > Guanghui Yue, Wanwan Han, Bin Jiang, Tianwei Zhou, <strong>Runmin Cong</strong>, and Tianfu Wang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9772424" target="_blank"><font color="#0000FF">Boundary constraint network with cross layer feature integration for polyp segmentation</font></a>,
  <ud2>IEEE Journal of Biomedical and Health Informatics</ud2>, vol. 26, no. 8, pp. 4090-4099, 2022. <br>
  <!--<a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 

	  
<li > Zhao Wang, Feng Li, <strong>Runmin Cong</strong>, Huihui Bai, and Yao Zhao, 
  <a href="https://link.springer.com/content/pdf/10.1007/s11042-022-12151-4.pdf" target="_blank"><font color="#0000FF">Adaptive feature fusion network based on boosted attention mechanism for single image dehazing</font></a>,
  <ud2>Multimedia Tools and Applications</ud2>, vol. 82, no. 1, pp. 11325-11339, 2022. <br>
</li> 	
	  
<li >  Weiqing Yan, Kaiqi Su,Jinlai Ren, <strong>Runmin Cong</strong>, Shuai Li, and Shuigen Wang,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Sparse LiDAR and binocular stereo fusion network for 3D object detection</font></a>,
  <ud2>The 5th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</ud2>, 2022. <br>
</li> 	  
	
	 
	  </ul>
  <hr />
	
	
	
<h4><b>2021:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
 
  
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu, Dingwen Zhang, and Qingming Huang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8998588" target="_blank"><font color="#0000FF">ASIF-Net: Attention steered interweave fusion network for RGBD salient object detection</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 50, no. 1, pp. 88-100, 2021. (# co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCyb (2020/12; 2021/01; 2021/03; 2021/04; 2021/06; 2021/12)</i></font></strong><br>
  <a href="https://github.com/Li-Chongyi/ASIF-Net"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"><font color="#FF5151">[Results]</font></a>-->
</li>
	  
<li > Qijian Zhang, <strong>Runmin Cong#</strong>, Chongyi Li, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434" target="_blank"><font color="#0000FF">Dense attention fluid network for salient object detection in optical remote sensing images</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 1305-1317, 2021. (CCF A, # co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Hot Paper, ESI Highly Cited Paper, Popular Documents in TIP (2021/01)</i></font></strong> <br>
  <a href="https://rmcong.github.io/proj_DAFNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/EORSSD-dataset"  target="_blank"><font color="#FF5151">[EORSSD Dataset]</font></a>
  <a href="https://github.com/rmcong/DAFNet_TIP20"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
<a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li> 
	
	
<li > Zuyao Chen^, <strong>Runmin Cong^</strong>, Qianqian Xu, and Qingming Huang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9247470" target="_blank"><font color="#0000FF">DPANet: Depth potentiality-aware gated attention network for RGB-D salient object detection</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 7012-7024, 2021. (CCF A, ^ equal contribution)  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_DPANet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
	<a href="https://www.bilibili.com/video/BV1Ry4y1m7WL/"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li> 	  
	  
<li > Chen Zhang, <strong>Runmin Cong*</strong>, Qinwei Lin, Lin Ma, Feng Li, Yao Zhao, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475364" target="_blank"><font color="#0000FF">Cross-modality discrepant interaction network for RGB-D salient object detection</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2094-2102, 2021. (CCF A, * corresponding author, VALSE'2021 Spotlight)<br>
  <a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
<a href="https://github.com/1437539743/CDINet-ACM-MM21"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
</li> 	  
	  
<li > Qi Tang, <strong>Runmin Cong*</strong>, Ronghui Sheng, Lingzhi He, Dan Zhang, Yao Zhao, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475373" target="_blank"><font color="#0000FF">BridgeNet: A joint learning network of depth map super-resolution and monocular depth estimation</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2148-2157, 2021. (CCF A, * corresponding author)<br>
  <a href="https://rmcong.github.io/proj_BridgeNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	   
	  
<li > Dong Jing, Shuo Zhang, <strong>Runmin Cong</strong>, and Youfang Lin,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475312" target="_blank"><font color="#0000FF">Occlusion-aware bi-directional guided network for light field salient object detection</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 1692-1701, 2021. (CCF A)
   <a href="https://github.com/Timsty1/OBGNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br
</li> 	 

<li > Hongfa Wen, Chenggang Yan, Xiaofei Zhou, <strong>Runmin Cong</strong>, Yaoqi Sun, Bolun Zheng, Jiyong Zhang, Yongjun Bao, and Guiguang Ding,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9605221" target="_blank"><font color="#0000FF">Dynamic selective network for RGB-D salient object detection</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 9179-9192, 2021. (CCF A) <strong><font color="#00CC33"><i>Popular Documents in TIP (2021/11)</i></font></strong>
  <a href="https://github.com/Brook-Wen/DSNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <!--<font color="#999999">[Code]</font>-->
</li> 	 
 
	  
<li > Chongyi Li, Saeed Anwar, Junhui Hou, <strong>Runmin Cong</strong>, Chunle Guo, and Wenqi Ren,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9426457" target="_blank"><font color="#0000FF">Underwater image enhancement via medium transmission-guided multi-color space embedding</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 4985-5000, 2021. (CCF A) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2021/05-2021/07; 2021/09-2022/10)</i></font></strong> <br>
  <a https://li-chongyi.github.io/Proj_Ucolor.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
</li> 	 
	  
	  
<li > Hua Li, Yuheng Jia, <strong>Runmin Cong</strong>, Sam Kwong, and Chuanbo Chen,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292086" target="_blank"><font color="#0000FF">Superpixel segmentation based on spatially constrained subspace clustering</font></a>,
  <ud2>IEEE Transactions on Industrial Informatics</ud2>, vol. 17, no. 11, pp. 7501-7512, 2021. <br>
   <!--<a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
</li> 	  
	
<li > Hua Li, <strong>Runmin Cong*</strong>, Sam Kwong, Chuanbo Chen, Qianqian Xu, and Chongyi Li,
  <a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025521X00023/1-s2.0-S002002552031197X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIAygFBH2m%2B%2Fb52oukNEDOg9vBCQlwVbi1%2BS7Sg9oJSnIAiEA074uSp3%2FDklNmIBwduQ3Ru2%2FABnoiJlUPtqrHK86iNcqtAMIahADGgwwNTkwMDM1NDY4NjUiDE32MwlqAAmZEkI%2F9SqRAxYD9DTwYcm6DxpmygwQTUoy9ndEPxUM%2FXuTChzQP11LAEzmnL26RWw6U6lAHnZVnQb%2B%2Fc%2FVAFHejQzhD8YDCky3sPAzOmqpXVXFHls7QMG2LgGhhuWDQpspfpRhS2MloLePIqVzv15WGH0%2BYIyKwebHPp1ar8tMiNEYQoCHUBz%2B6WQQcQzhEJZoQPLrHQDQNMa59b9tZV9DTm7fU6UJ4TELPT4iHhYujxFZxuKPcmMUvG28I5eau5wIdb%2BCCMngJRHpaTyuzJ70c7p01hafkMZhbznzOL0OWvzfLWHh4na8UxNrnr5SW5oMDE0eSqgSnpwD4yDAWvTfsPehvPuBJyKZWMCd7qyJLRXvr5yOzIXTBGLUzUKaCtJLgS8MUcLTpFHKszUt2dFHjMEEB8pb3BZrqr2XobKwPSekS0krP28wSFA5pAHrnfMWliyj9kZBnyVdurnjmcOrSknUx1dDonY6GCPpCVSHrGH6qNN8gUwmPpRnJxPNNRjo4GCFuXLHdY8qxQgiQmbUzJej%2F0nBHXSDMN3imIAGOusBq1tMJqeUTsrIVpUUtv9GU5gpG8%2FK%2F5gHthUou7%2Bfx8aTyuRWUcZ9GF1nhHPFI49MXUiGRbW9qS3UWnTfrzYNl9e5hoTof5jr7CK74BlzDBRHqHRVYweYNr6PpkZopDSvorW40zAzwpBxXIMA1XSLIgntBZNTqBMwiWea9ulQek6fVLkV0RmU357N7Ets15p7ivDDoU2ZfyE%2BiXEp5c2%2FBFME%2BYeYavuOxBEeSZH5AaCyUqCjM8oX98%2Br4%2BTpWz8GzPMOYmKwg69u7gYZoPS4GXXMzD0FNzhXgc8QECZQm%2BD5fRSEH75WSKOOQA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210119T015339Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZVTASF26%2F20210119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0bb9b9266999431dfd680d53ccfc58cfcd456f8bf9f75c122380c6260de1cebe&hash=829bfa37bd56fea063d166b49a7466f05c59b7570955e5014ab9d02c8c60338a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S002002552031197X&tid=spdf-03751d9c-ee4f-4c51-8986-c6a1f06bcf9a&sid=2743a5db222cd54e78095df16e98e6012296gxrqa&type=client" target="_blank">
	  <font color="#0000FF">Stereo superpixel: An iterative framework based on parallax consistency and collaborative optimization</font></a>,
  <ud2>Information Sciences</ud2>, vol. 556, pp. 209-222, 2021. (* corresponding author) <br>
   <!--<a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
</li> 	
	  
	  
<li > Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, <strong>Runmin Cong</strong>, Chunjie Zhang, Chunyu Lin, Meiqin Liu, and Yao Zhao,
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_Towards_Fast_and_Accurate_Real-World_Depth_Super-Resolution_Benchmark_Dataset_and_CVPR_2021_paper.pdf" target="_blank"><font color="#0000FF">Towards fast and accurate real-world depth super-resolution: Benchmark dataset and baseline</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 9229-9238, 2021. (CCF A, VALSE'2021 Spotlight) <br>
   <a href="http://mepro.bjtu.edu.cn/resource.html"  target="_blank"><font color="#FF5151">[RGB-D-D Dataset]</font></a>
   <a href="https://github.com/lingzhi96/RGB-D-D-Dataset"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li> 	

<li > Ning Yang, Qihang Zhong, Kun Li, <strong>Runmin Cong*</strong>, Yao Zhao, and Sam Kwong,
  <a href="https://pdf.sciencedirectassets.com/271519/1-s2.0-S0923596521X00033/1-s2.0-S0923596521000503/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBkaCXVzLWVhc3QtMSJIMEYCIQCZufipe4utlPRuvKkkM2335wR%2BcJ466YEGMmffQojgkgIhAJldmoqsTzrj28ZhX1rg3yCryOYuBNyChlnktI6RTzh%2FKrQDCFIQAxoMMDU5MDAzNTQ2ODY1IgxvUIVhIu2NaVYCIHMqkQNfU0g4vtc1K7bzI2uNv5OuV8BrIAvbPuxOUKMAPFSdmO9sMi6rKZiaVOslZpIra%2Fp54REUwAMFobL9F1g3xwyBNjNTmRRVYnWJuX6zOTMjtri9mG4qhzcH6307co3vuPJenqjtdDWBZt%2FPx3UUBvCNuKOFER4KyI5fNp%2Bv5QF19jA86%2BL2tO6XtlD5Ux3M3KRtpeZlGGfm5t9TNlOyEGllkQg9gJVBL8uCsAQmB84H0uCyYaHriGqDKz%2BgmXySlpfwLDkAF526m2A48%2BCreoNu4%2FeQ0N8nP7ifSuEV3w8ev9vWznJd80OH7o6FyStjbMjr8ZNPbnYDk5deupsVqN%2BTAHkDhCG%2B4EAZGTGzl26lU36RqjDvMz0ELWxunHWr%2B9I4pQUzGESoxakERYs%2BAoZg0fbduwF6Edn2FIQCR1aFV2T4tbTPkI6sre3ZjjXz7XGl69QojAKMA9%2FwIl%2FO%2BOZEHQZtnD13YWDlv07LWPQy%2BPmVgQVWzXIAfFUn5cKeCTRB5o70iu4QPFkq4Eg6zxXyLjDM7c%2BCBjrqAWJ8kSgwF7dnybnKYbakF9LI7W9IzjM0fjqHBzb1uQGyu8NUa8no850HN204MuhS5q7iK6xInN0U%2FlVaoE%2FfISOm%2FcFWaAgAYb8WEgQZ5TXm2sUnW%2FbfXTbhIJ54l9bcGCPom8Y5oWKCfPnvdxGFRPGMKfMrE5eIuXH2WEajpsLulUYyoMEAsyIeXQgNBQWxHZ1wCF9fZGV7eYKaogxI%2BHNTogOr%2F6SOg6el8%2FcHHqWyoPE%2BWlCbHs%2BNBYeP7IHJJJNAWUf3Wqvaxj4Ek%2Fox1OgDBIQxRRh%2F4%2FmWPJtxv8LLzkdjvVJIPNGrDw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210319T015224Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYQRBHFMGN%2F20210319%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=c3614abab9ddf93ae556038ab84299f41d1b96d17620256b45dd2c60092f4dcc&hash=0d54c4e2714038ea8368b3c3c9d650b5e101fda1da545c2f1abe6273ea0324e7&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0923596521000503&tid=spdf-0a645599-32f1-472c-aef7-40038f476093&sid=ec8c75165ab1a645ad398fb410c52314e2a1gxrqa&type=client">
	  <font color="#0000FF">A reference-free underwater image quality assessment metric in frequency domain</font></a>,
  <ud2>Signal Processing: Image Communication</ud2>, vol. 94, pp. 1-10, 2021. (* corresponding author) <br>
   <a href="https://pan.baidu.com/s/12GHKKUCRRoR1LzhsKcSNRQ"  target="_blank"><font color="#FF5151">[UWIQA Dataset] (code: MVPL)</font></a>
	 <a href="https://github.com/rmcong/Code-for-FDUM-method"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li>
													
<li > Xuan Liu, Yumo Zhang, <strong>Runmin Cong*</strong>, Chen Zhang, Ning Yang, Chunjie Zhang, and Yao Zhao,
  <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-88007-1_48.pdf" target="_blank"><font color="#0000FF">GGRNet: Global graph reasoning network for salient object detection in optical remote sensing images</font></a>,
  <ud2>The 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</ud2>, pp. 584–596, 2021. (* corresponding author)  <br>
</li> 	  
											   

<li > Junkang Hu, Qiuping Jiang, <strong>Runmin Cong</strong>, Wei Gao, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9496260" target="_blank"><font color="#0000FF">Two-branch deep neural network for underwater image enhancement in HSV color space</font></a>,
  <ud2>IEEE Signal Processing Letters</ud2>, vol. 28, pp. 2152-2156, 2021. <strong><font color="#00CC33"><i>Popular Documents in SPL (2022/09; 2022/10)</i></font></strong><br>
</li> 	  	  
	 
	  </ul>
  <hr />
	
	
<h4><b>2020:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">  

<li > <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Junhui Hou, Qingming Huang, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8807367" target="_blank"><font color="#0000FF">Going from RGB to RGBD saliency: A depth-guided transformation model</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 50, no. 8, pp. 3627-3639, 2020. 
  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-DTM-Method"><font color="#FF5151">[Code]</font></a></li>
		
<li > Qijian Zhang, <strong>Runmin Cong#</strong>, Junhui Hou, Chongyi Li, and Yao Zhao, 
  <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf" target="_blank"><font color="#0000FF">CoADNet: Collaborative aggregation-and-distribution networks for co-salient object detection</font></a>,
  <ud2>Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</ud2>, pp. 6959-6970, 2020. (CCF A, # co-first and corresponding author) <br>
  <a href="https://rmcong.github.io/proj_CoADNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"><font color="#FF5151">[Results]</font></a>-->
  <a href="https://github.com/rmcong/CoADNet_NeurIPS20"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
<a href="https://www.bilibili.com/video/BV1np4y1z7B7"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li>
	
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Yongri Piao, Qianqian Xu, and Chen Change Loy, 
  <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf" target="_blank"><font color="#0000FF">RGB-D salient object detection with cross-modality modulation and selection</font></a>,
  <ud2>European Conference on Computer Vision (ECCV)</ud2>, pp. 225-241, 2020. (# co-first and corresponding author) <br>
  <a href="https://li-chongyi.github.io/Proj_ECCV20"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <a href="https://github.com/Li-Chongyi/cmMS-ECCV20"  target="_blank"><font color="#FF5151">[Code]</font></a>
  </li>
	
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, 
  <a href="https://www.sciencedirect.com/science/article/pii/S0925231220313692?dgcid=author" target="_blank"><font color="#0000FF">A parallel down-up fusion network for salient object detection in optical remote sensing images</font></a>,
  <ud2>Neurocomputing</ud2>, vol. 415, pp. 411-420, 2020. (# co-first and corresponding author) <br>
  <!--<a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>
  <font color="#999999">[Results]</font>--> 
</li>
	

<li > Chongyi Li, Huazhu Fu, <strong>Runmin Cong*</strong>, Zechao Li, and Qianqian Xu, 
  <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413928" target="_blank"><font color="#0000FF">NuI-Go: Recursive non-local encoder-decoder network for retinal image non-uniform illumination removal</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 1478-1487, 2020. (CCF A, * corresponding author) <br>
  <a href="https://li-chongyi.github.io/Proj_ACMMM20_NuI-Go.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  </li>

<li > Peisong Wen, Ruolin Yang, Qianqian Xu, Chen Qian, Qingming Huang, <strong>Runmin Cong</strong>, and Jianlou Si, 
  <a href="https://dl.acm.org/doi/10.1145/3394171.3414035" target="_blank"><font color="#0000FF">DMVOS: Discriminative matching for real-time video object segmentation</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2048-2056, 2020. (CCF A)<br>
  <!--<a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>
	
	
<li > Yawen Huang, Feng Zheng, <strong>Runmin Cong</strong>, Weilin Huang, Matthew R. Scott, and Ling Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9152126" target="_blank"><font color="#0000FF">MCMT-GAN: Multi-task coherent modality transferable GAN for 3D brain image synthesis</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 29, pp. 8187-8198, 2020. <br> 
</li>

<li > Feng Li^, <strong>Runmin Cong^</strong>, Huihui Bai, and Yifan He, 
  <a href="https://www.ijcai.org/Proceedings/2020/0075.pdf" target="_blank"><font color="#0000FF">Deep interleaved network for image super-resolution with asymmetric co-attention</font></a>,
  <ud2>International Joint Conference on Artificial Intelligence (IJCAI)</ud2>, pp. 534-543, 2020. (CCF A, ^ equal contribution) <br>
  <a href="https://github.com/lifengshiwo/DIN"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li>
	
<li> Ping Han, Binbin Han, Xiaoguang Lu, <strong>Runmin Cong*</strong>, and Dandan Sun, 
  <a href="https://www.tandfonline.com/doi/full/10.1080/01431161.2019.1643939" target="_blank"><font color="#0000FF">Unsupervised classification of PolSAR images based on multi-level feature extraction</font></a>, 
  <ud2>International Journal of Remote Sensing</ud2>, vol. 41, no. 2, pp. 534-548, 2020. (* corresponding author)  <br></li>	  

<li > Chongyi Li, Chunle Guo, Wenqi Ren, <strong>Runmin Cong</strong>, Junhui Hou, Sam Kwong, and Dacheng Tao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8917818" target="_blank"><font color="#0000FF">An underwater image enhancement benchmark dataset and beyond</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 29, pp. 4376-4389, 2020.  <strong><font color="#00CC33"><i>ESI Hot Paper, ESI Highly Cited Paper, Popular Documents in TIP (2020/04-2022/10)</i></font></strong><br>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[UIEBD Dataset]</font></a>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[Code and Results]</font></a><br></li>

<li> Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and <strong>Runmin Cong</strong>, 
  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank"><font color="#0000FF">Zero-reference deep curve estimation for low-light image enhancement</font></a>, 
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 1780-1789, 2020. (CCF A) 
  <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/Li-Chongyi/Zero-DCE"><font color="#FF5151">[Code]</font></a></li>
			
<li> Zuyao Chen, Qianqian Xu, <strong>Runmin Cong</strong>, and Qingming Huang, 
  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6633" target="_blank"><font color="#0000FF">Global context-aware progressive aggregation network for salient object detection</font></a>, 
  <ud2>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</ud2>, pp. 10599-10606, 2020. (CCF A, <font color="#FF5151">Oral Presentation</font>) 
  <a href="https://github.com/JosephChenHub/GCPANet"><font color="#FF5151">[Code]</font></a></li>
			
<li> Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8792133" target="_blank"><font color="#0000FF">PDR-Net: Perception-inspired single image dehazing network with refinement</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 22, no. 3, pp. 704-716, 2020. <strong><font color="#00CC33"><i>Popular Documents in TMM (2020/03-2022/05; 2020/07-2020/09; 2020/12)</i></font></strong><br></li>
			
<li> Mengxin Han, <strong>Runmin Cong</strong>, Xinyu Li, Huazhu Fu, and Jianjun Lei, 
  <a href="https://reader.elsevier.com/reader/sd/pii/S0167865518307748?token=72E89BF2F7EAA411988F3F266DD1A9872321E794C0F05651EF5297ACB414955DDF0FA962EB3DD0403CBDC0CC9A11EDFC" target="_blank"><font color="#0000FF">Joint spatial-spectral hyperspectral image classification based on convolutional neural network</font></a>,
  <ud2>Pattern Recognition Letters</ud2>, vol. 130, pp. 38-45, 2020. <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br></li>
			
<li> Ling Du, Anthony T.S. Ho, and <strong>Runmin Cong</strong>, 
  <a href="https://pdf.sciencedirectassets.com/271519/1-s2.0-S0923596519X00107/1-s2.0-S0923596519301286/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJIMEYCIQCN2vQCvUAL01W5V7K%2FnMV3sjrPNJMAFD0fLhlvaMT3kQIhAKn4XPKexGavuJYT0NXPDLrJl9MZkirT9beXgRDVi9OXKrQDCBMQAhoMMDU5MDAzNTQ2ODY1IgxPak1%2BdhrHClpchp8qkQM4BGURXfAFxKW7eVxcX6HuivgNZG7xUz1VIeLDy1CripP2DQhNb94zRLxHF8zmYnfjlw2dgCsGLWYKwWdTaeFVt6CMr4osKu%2BGS5jPV30PM9WJ3wb39eCf9GdlkjF6ChfNNaKok8K9deAD4czJDfl5JWD027lAikfHMgiK0DIEOVIrn6o6p%2B1EeLZbBdjhKjJDXJrNNaP7tLgxN%2FJtgIz%2FV6w2SYnzIBM36iF1%2Fw6Zn1Wzeqq1LOrcTAB0yid2Tspg84bibCs4APBsCU99WEvaKmBwW2tDlyArqGdm9ZC4zSCySvlnF3KyDCpUyeukAQy%2Bt1mMOJrKIXSyhLBOJ%2BiOe9El0p54zOUtfXn4Z%2BLwtG38HHzFepTsv7zueM4xZgG87cgE0F4cO1qFhvrLM6HPUjZknG2%2FN3eTbVgGcGcZ9TRXBZNFkibeEwqTlD89gtfngrQIRr4%2BYDnSHGor7OOMnO6V9fI1ADkxiTQS%2Bq5i1rK3KAcGc6OlWd5nL7%2BQefU%2BeUBut4PuniIX62qCurXOlzCHl%2B7yBTrqAbZNeL4Ts5ZDExDd1uvAlwIbiKlsrMYGsbFdbG0jJarammkD5%2Fsv7fe7GHj8VB5Cpo9cNTC8682p2Rf1Wkv%2FJy7kFLTAM9G5iNG8SmaA2%2BML2OW0%2FRDogjMAH%2Bj1V0RktBRc6QhwiB4bZlIjtSMb18u69h1wNaTrQLEpK4cjWrKhE1GUpbHJdeZvqCd4xjcr99wqdKz5lw7q24Kzkx4C2cosq1snLcQAvP12gudaeI6qHBgQUqOzKDaSFYzr4EC%2Bvf12t7B872BYkE93z0KMd6Gd0PoD72PLu1woGXgW%2BWGQ7abbBnyWXPebCQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200301T105922Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRI43AGMU%2F20200301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b2fb4a7682437c2bde4361f6284b553747c19ac05327209f2733727799fe8376&hash=9eaa2b4f295be43fde971320b8ea29401b403055a2f1b600861747f129cb1faa&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0923596519301286&tid=spdf-a1effdbd-edfc-4cf9-bb67-a07c54b9ba77&sid=42f01d2749d9e849294bb514e02324dda678gxrqa&type=client" target="_blank"><font color="#0000FF">Perceptual hashing for image authentication: A survey</font></a>, 
  <ud2>Signal Processing: Image Communication</ud2>, vol. 81, pp. 1-23, 2020. <br></li>	  
		
<li> <strong>丛润民</strong>, 张禹墨, 张晨, 李重仪, 赵耀,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">深度学习驱动的水下图像增强与复原研究进展</font></a>, 
  <ud2>信号处理</ud2>, 36(9): 1377-1389, 2020. <br></li>	  

  
	</ul>
			<hr />
				
<h4><b>2019:</b></h4>
<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
<li > <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Weisi Lin, Qingming Huang, Xiaochun Cao, and Chunping Hou, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8116754" target="_blank"><font color="#0000FF">An iterative co-saliency framework for RGBD images</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 49, no. 1, pp. 233-246, 2019. <br/>
  <a href="https://rmcong.github.io/proj_RGBD_cosal_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/2017-TCyb-An-iterative-RGBD-co-saliency-framework"  target="_blank"><font color="#FF5151">[Results]</font></a><br></li>

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Fatih Porikli, Qingming Huang, and Chunping Hou,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8704996" target="_blank"><font color="#0000FF">Video saliency detection via sparsity-based reconstruction and propagation</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 28, no. 10, pp. 4819-4831, 2019. <strong><font color="#00CC33"><i>Popular Documents in TIP (2019/08)</i></font></strong><br></li>
  <a href="https://rmcong.github.io/proj_video_sal_SRP_tip.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-SRP-Method"><font color="#FF5151">[Code and Results]</font></a>
  <a href="demo/video_sal_SRP_demo.avi"  target="_blank"><font color="#FF5151">[Demo]</font></a>  
	  
<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, and Nam Ling, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8556071" target="_blank"><font color="#0000FF">HSCS: Hierarchical sparsity based co-saliency detection for RGBD images</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 21, no. 7, pp. 1660-1671, 2019. <br/>
  <a href="https://rmcong.github.io/proj_RGBD_cosal_HSCS_tmm.html" target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-HSCS-Method"><font color="#FF5151">[Code]</font></a>
  <a href="https://github.com/rmcong/Results-for-2018TMM-HSCS" target="_blank"><font color="#FF5151">[Results]</font></a><br></li>

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng, Weisi Lin, and Qingming Huang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466906" target="_blank"><font color="#0000FF">Review of visual saliency detection with comprehensive information</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 29, no. 10, pp. 2941-2959, 2019. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCSVT (2019/10-2020/01; 2020/03; 2020/06; 2021/01; 2021/03; 2021/05; 2021/06; 2021/11; 2022/03)</i></font></strong><br></li>  
    
<li> Chongyi Li, <strong>Runmin Cong#</strong>, Junhui Hou, Sanyi Zhang, Yue Qian, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793227" target="_blank"><font color="#0000FF">Nested network with two-stream pyramid for salient object detection in optical remote sensing images</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 57, no. 11, pp. 9156-9166, 2019. (# co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong> <br>
  <a href="https://li-chongyi.github.io/proj_optical_saliency.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/ORSSD-dataset"  target="_blank"><font color="#FF5151">[ORSSD Dataset]</font></a>
<a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>
  	  
<li> Chunle Guo, Chongyi Li, Jichang Guo, <strong>Runmin Cong</strong>, Huazhu Fu, and Ping Han, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8579111" target="_blank"><font color="#0000FF">Hierarchical features driven residual learning for depth map super-resolution</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 28, no. 5, pp. 2545-2557, 2019. 
  <a href="https://li-chongyi.github.io/proj_SR.html" target="_blank"><font color="#FF5151">[----Project Page----]</font></a></li>

<li> Hua Li, Sam Kwong, Chuanbo Chen, Yuheng Jia, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8673630" target="_blank"><font color="#0000FF">Superpixel segmentation based on square-wise asymmetric segmentation and structural approximation</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 21, no. 10, pp. 2625-2637, 2019. <br></li>
 	  
	  
	  
</ul>
	<hr />
  
<h4><b>2018:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, and Chunping Hou,
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8070326" target="_blank" ><font color="#0000FF">Co-saliency detection for RGBD images based on multi-constraint feature matching and cross label propagation</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 27, no. 2, pp. 568-579, 2018. <br>
  <a href="https://rmcong.github.io/proj_RGBD_cosal.html" target="_blank" ><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/RGBD-Cosal150-Dataset"  target="_blank" ><font color="#FF5151">[Dataset]</font></a>
  <a href="https://github.com/rmcong/Results-for-2018TIP-RGBD-Co-saliency"  target="_blank" ><font color="#FF5151">[Results]</font></a><br></li>   

<li> <strong>丛润民</strong>, 雷建军, 付华柱, 王文冠, 黄庆明, 牛力杰,
  <a href="http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5560&journal_id=jos" target="_blank"><font color="#0000FF">视频显著性检测研究进展</font></a>,
  <ud2>软件学报</ud2>, 29(8): 2527−2544, 2018. (CCF A, EI, <font color="#FF5151"><strong>第十五届北京青年优秀科技论文奖</strong></font>)<br></li>

<li> Yonghua Zhang, Liang Li, <strong>Runmin Cong</strong>, Xiaojie Guo, Hui Xu, and Jiawan Zhang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8486603" target="_blank" ><font color="#0000FF">Co-saliency detection via hierarchical consistency measure</font></a>, 
  <ud2>IEEE International Conference on Multimedia & Expo (ICME)</ud2>, pp. 1-6, 2018. <font color="#FF5151">Oral Presentation, <strong>Best Student Paper Runner-Up (rate: 2/582)</strong></font><br></li> 
  </ul>
  <hr />
			
<h4><b>2017:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">  

<li> Min Ni, Jianjun Lei, <strong>Runmin Cong*</strong>, Kaifu Zheng, Bo Peng, and Xiaoting Fan, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8106766" target="_blank"><font color="#0000FF">Color-guided depth map super resolution using convolutional neural network</font></a>, 
  <ud2>IEEE Access</ud2>, vol. 2, pp. 26666-26672, 2017. (* corresponding author) <br></li>
  
<li> Chongyi Li, Jichang Guo, Chunle Guo, <strong>Runmin Cong</strong>, and Jiachang Gong, 
  <a href="https://reader.elsevier.com/reader/sd/pii/S016786551730171X?token=6A97A6E5D8235CD901B3FCDDA0C23B76726B7DA66091936F46C1DF69DB05A08CB47410E83AC3BC041DAFF11B0254B376" target="_blank"><font color="#0000FF">A hybrid method for underwater image correction</font></a>,
  <ud2>Pattern Recognition Letters</ud2>, vol. 94, pp. 62-67, 2017.  <br></li>
  </ul>
			<hr />
			
<h4><b>2016:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	
<li> <strong>Runmin Cong</strong>, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, and Chunping Hou, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7457641" target="_blank"><font color="#0000FF">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</font></a>, 
  <ud2>IEEE Signal Processing Letters</ud2>, vol. 23, no. 6, pp. 819-823, 2016. <strong><font color="#00CC33"><i>Popular Documents in SPL (2016/05-2016/10)</i></font></strong> <br>
  <a href="https://rmcong.github.io/proj_RGBD_sal.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-DCMC-method"  target="_blank"><font color="#FF5151">[Code]</font></a><br></li>
  
<li> <strong>Runmin Cong</strong>, Ping Han, Chongyi Li, Jiaji He, and Zaiji Zhang, 
  <a href="https://www.researchgate.net/publication/308665367_Manmade_target_extraction_based_on_multistage_decision_and_its_application_for_change_detection_in_polarimetric_synthetic_aperture_radar_image?_iepl%5BviewId%5D=NimQuYvO7KOlf738gXMw8OZYLNFxI1Mop5xZ&_iepl%5Bcontexts%5D%5B0%5D=prfhpi&_iepl%5Bdata%5D%5BstandardItemCount%5D=3&_iepl%5Bdata%5D%5BuserSelectedItemCount%5D=0&_iepl%5Bdata%5D%5BtopHighlightCount%5D=2&_iepl%5Bdata%5D%5BstandardItemIndex%5D=1&_iepl%5Bdata%5D%5BstandardItem1of3%5D=1&_iepl%5BtargetEntityId%5D=PB%3A308665367&_iepl%5BinteractionType%5D=publicationTitle" target="_blank"><font color="#0000FF">Manmade target extraction based on multi-stage decision and its application for change detection in polarimetric synthetic aperture radar image</font></a>,
  <ud2>Journal of Electronic Imaging</ud2>, vol. 25, no. 5, pp. 1-13, 2016. <br></li> 
  
<li> Chongyi Li, Jichang Guo, <strong>Runmin Cong</strong>, Yanwei Pang, and Bo Wang, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7574330" target="_blank"><font color="#0000FF">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 25, no. 12, pp. 5664-5677, 2016. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2022/10)</i></font></strong>
  <a href="https://github.com/Li-Chongyi/TIP2016-code"  target="_blank"><font color="#FF5151">[Code]</font></a></li>
  
<li> Chongyi Li, Jichang Guo, Bo Wang, <strong>Runmin Cong</strong>, Yan Zhang, and Jian Wang, 
  <a href="https://www.researchgate.net/publication/303888472_Single_underwater_image_enhancement_based_on_color_cast_removal_and_visibility_restoration" target="_blank"><font color="#0000FF">Single underwater image enhancement based on color cast removal and visibility restoration</font></a>, 
  <ud2>Journal of Electronic Imaging</ud2>, vol. 25, no. 3, pp. 1-16, 2016. <br>
  </li>
</ul>
			<hr />
<!--
<h4><b>2015:</b></h4>
		<ul class="graid3-ul">
 <div style="text-align: justify; display: block; margin-right: auto;">
	 
<li> Ping Han, <strong>Runmin Cong</strong>, and Zaiji Zhang, 
  <font color="#0000FF">Change detection algorithm of polarimetric SAR image based on polarization state extracting</font>,
  <ud2>Systems Engineering and Electronics</ud2>, vol. 37, no. 7, pp. 1526-1530, 2015. (in Chinese, EI)<br></li>
  </ul>
				<hr />
-->
			
<h4><b>授权中国发明专利:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	 
<li>一种立体视觉显著性检测方法，专利号：ZL 201610244589.9，申请日：2016.04.19，授权公告日：2018.08.31</li>
<li>一种图间显著性检测方法，专利号：ZL 201710942099.0，申请日：2017.10.11，授权公告日：2021.04.16</li>
<li>一种深度图可靠性评价测度方法，专利号：ZL 201610242241.6，申请日：2016.04.19，授权公告日：2018.08.10</li>	 
<li>一种立体图像重定向方法，专利号：ZL 201610874827.4，申请日：2016.09.30，授权公告日：2019.12.06</li>
<li>一种迭代协同显著性检测方法，专利号：ZL 201711064083.0，申请日：2017.11.02，授权公告日：2021.06.04</li>
<li>一种协同显著性检测方法，专利号：ZL 201710942783.9，申请日：2017.10.11，授权公告日：2021.06.04</li>
<li>一种深度形状先验提取方法，专利号：ZL 201711065005.2，申请日：2017.11.02，授权公告日：2021.04.30</li>
<li>一种 RGBD 图像协同显著性检测方法，专利号：ZL 201810879724.6，申请日：2018.08.03，授权公告日：2021.09.17</li>
<li>一种深度视频快速帧内编码方法，专利号：ZL 201810317701.6，申请日：2018.04.10，授权公告日：2021.04.30</li>
<li>一种联合场景和运动多特征的视频行为聚类方法，专利号：ZL 201810962264.3，申请日：2018.08.22，授权公告日：2021.06.04</li>
<li>深度图超分辨率重建方法，专利号：ZL 201610727602.6，申请日：2016.08.25，授权公告日：2019.10.18</li>	
<li>一种2D转3D深度估计方法，专利号：ZL 201610780883.1，申请日：2016.08.31，授权公告日：2019.06.04</li>	 	 
<li>一种立体图像匹配图计算方法，专利号：ZL 201610780786.2，申请日：2016.08.31，授权公告日：2019.05.31</li>	
<li>一种基于最优化颜色修正和回归模型的水下图像复原方法，专利号：ZL 201610606187.9，申请日：2016.07.25，授权公告日：2019.03.29</li>  
<li>一种屏幕内容与自然内容划分及快速编码方法，专利号：ZL 201611031480.3，申请日：2016.11.18，授权公告日：2019.01.29</li>
<li>一种基于虚拟视点绘制质量的深度图上采样方法，专利号：ZL 201610751851.9，申请日：2016.08.27，授权公告日：2019.08.02</li>	 
	 
	<!-- 
<li><strong>丛润民</strong>，雷建军，侯春萍，李重仪，贺小旭，段金辉. 一种立体视觉显著性检测方法，专利号：ZL 201610244589.9，申请日：2016.04.20，授权公告日：2018.08.31
</li>

<li>雷建军，<strong>丛润民</strong>，侯春萍，段金辉，李东阳. 一种深度图可靠性评价测度，专利号：ZL 201610242241.6，申请日：2016.04.20，授权公告日：2018.08.10
 </li>	 

<li>雷建军，<strong>丛润民</strong>，侯春萍，张三义，陈越，郭琰. 一种迭代协同显著性检测方法，专利号：ZL 201711064083.0，申请日：2017.11.02，授权公告日：2021.06.04</li>
<li>雷建军，<strong>丛润民</strong>，侯春萍，张静，范晓婷，彭勃. 一种协同显著性检测方法，专利号：ZL 201710942783.9，申请日：2017.10.11，授权公告日：2021.06.04</li>
<li>雷建军，<strong>丛润民</strong>，侯春萍，李欣欣，韩梦芯，罗晓维. 一种深度形状先验提取方法，专利号：ZL 201711065005.2，申请日：2017.11.02，授权公告日：2021.04.30</li>
<li>雷建军，张凯明，孙振燕，彭勃，<strong>丛润民</strong>，张曼华，徐遥令. 一种深度视频快速帧内编码方法，专利号：ZL 201810317701.6，申请日：2018.04.10，授权公告日：2021.04.30</li>
<li>雷建军，彭勃，郑泽勋，贾亚龙，<strong>丛润民</strong>，张静. 一种联合场景和运动多特征的视频行为聚类方法，申请号：201810962264.3，申请日：2018.08.22，授权公告日：2021.06.04</li>


<li>郭继昌，李重仪，<strong>丛润民</strong>，郭春乐，顾翔元.一种基于最优化颜色修正和回归模型的水下图像复原方法，专利号：ZL 201610606187.9，申请日：2016.07.25，授权公告日：2019.03.29
 </li> 

<li>雷建军，吴敏，侯春萍，<strong>丛润民</strong>，李乐乐，郭琰. 一种立体图像重定向方法，专利号：ZL 201610874827.4，申请日：2016.09.30，授权公告日：2019.12.06
 </li>
	 
<li>雷建军，李乐乐，侯春萍，<strong>丛润民</strong>，张凝，吴敏. 一种基于虚拟视点绘制质量的深度图上采样方法，专利号：ZL 201610751851.9，申请日：2016.08.27，授权公告日：2019.08.02
 </li>	 
	 
<li>雷建军，李乐乐，侯春萍，吴敏，<strong>丛润民</strong>，倪敏. 深度图超分辨率重建方法，专利号：ZL 201610727602.6，申请日：2016.08.25，授权公告日：2019.10.18
</li>	
	 
<li>吴敏，雷建军，侯春萍，李乐乐，<strong>丛润民</strong>，梅旭光. 一种立体图像匹配图计算方法，专利号：ZL 201610780786.2，申请日：2016.08.31，授权公告日：2019.05.31
</li>	
	 
<li>雷建军，李东阳，侯春萍，孙振燕，<strong>丛润民</strong>，彭勃. 一种屏幕内容与自然内容划分及快速编码方法，专利号：ZL 201611031480.3，申请日：2016.11.18，授权公告日：2019.01.29
   </li>
	 
<li>雷建军，张凝，侯春萍，张翠翠，郑凯夫，<strong>丛润民</strong>. 一种2D转3D深度估计方法，专利号：ZL 201610780883.1，申请日：2016.08.31，授权公告日：2019.06.04
 </li>	 	 
	--> 
  </ul>	
	<hr />
			
<ul class="graid3-ul">
        <div style="text-align: justify; display: block; margin-right: auto;">
	Here are the Impact Factors (IF) of selected journals of my publications. <br>
        - IEEE Transactions on Cybernetics (TCyb): 19.118 <br>
	- IEEE Transactions on Neural Networks and Learning Systems (TNNLS): 14.255<br>
	- IEEE Transactions on Industrial Informatics (TII): 11.648<br>
	- IEEE Transactions on Image Processing (TIP): 11.041 <br>
	- IEEE Transactions on Intelligent Transportation Systems (TITS): 9.551  <br>
	- IEEE Transactions on Multimedia (TMM): 8.182 <br> 
	- IEEE Transactions on Geoscience and Remote Sensing (TGRS): 8.125 <br>
	- IEEE Transactions on Circuits and Systems for Video Technology (TCSVT): 5.859 <br>
	- IEEE Transactions on Instrumentation and Measurement (TIM): 5.332 <br>
	- IEEE Transactions on Computational Imaging (TCI): 4.708 <br>
	- IEEE Transactions on Consumer Electronics (TCE): 4.414 <br>
	- IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI): 4.851 <br>
	- IEEE Journal of Biomedical and Health Informatics (JBHI): 7.021<br>
	- SCIENCE CHINA Information Sciences (SCIS): 7.275<br>
	- ACM Transactions on Multimedia Computing Communications and Applications (TOMM): 4.094<br>
	- Information Sciences: 8.233 <br>
	- Neurocomputing: 5.779 <br>
	- Pattern Recognition Letters (PRL): 4.757 <br>
	- IEEE Signal Processing Letters (SPL): 3.201 <br>
	- International Journal of Remote Sensing (IJRS): 3.531	 <br>
	- Signal Processing: Image Communication (SPIC): 3.453 <br>
	- Multimedia Tools and Applications (MTAP): 2.577 <br>
	- Journal of Electronic Imaging (JEI): 0.829<br>
	
	
			
			
	</div>			
		</div>	
	</div>
	
		<script type="text/javascript" src="./files/jquery.min.js.下载"></script>
		<script type="text/javascript" src="./files/bootstrap.js.下载"></script>
		<script type="text/javascript" src="./files/jquery.banner.js.下载"></script>
		<script type="text/javascript" src="./files/jquery.prettyPhoto.js.下载"></script>		
		<script type="text/javascript" src="./files/jquery.isotope.js.下载"></script>	
		<script type="text/javascript" src="./files/main.js.下载"></script>				
	


</body></html>
