<!DOCTYPE html>
<!-- saved from url=(0057)http://www.icst.pku.edu.cn/struct/people/yangs/index.html -->
<html class="csstransforms csstransforms3d csstransitions"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		
		<title>Runmin Cong</title>
		<link href="./files/bootstrap.css" rel="stylesheet" type="text/css" media="all">
		<link href="./files/style.css" rel="stylesheet" type="text/css" media="all">
		<link href="./files/prettyPhoto.css" rel="stylesheet" type="text/css" media="all">		
		<link href="./files/css" rel="stylesheet" type="text/css">	
	</head>
	<body>
		<!---start-wrap--->
		<!---start-header--->
	<div class="header">
		<div class="wrap">
			<!---start-logo--->
			<div class="logo">
				<a href="./index.html">Runmin Cong</a>
			</div>
			<!---End-logo--->
			<!---start-top-nav--->
			<div class="top-nav">
				<ul>
					<li id="Home" onclick="func(&#39;Me&#39;)"><a href="./index.html" class="scroll">Home</a></li>
					<li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="./Profile.html" class="scroll">Profile</a></li>
					<li id="Publications" onclick="func(&#39;Publications&#39;)"><a href="./Publication.html" class="scroll">Publications</a></li>
					<!--<li id="Profile-chinese" onclick="func(&#39;Honors Awards&#39;)"><a href="./Honors Awards.html" class="scroll">Honors & Awards</a></li>-->
					<li id="MVPLab" onclick="func(&#39;MVP Lab&#39;)"><a href="./MVPLab.html" class="scroll">MVP Group</a></li>
                                        <!--<li id="Projects" onclick="func(&#39;Projects&#39;)"><a href="./Project.html" class="scroll">Projects</a></li>-->
					<!-- <li id="Art" onclick="func(&#39;Art&#39;)"><a href="http://www.icst.pku.edu.cn/struct/people/yangs/index.html#art" class="scroll">Art Gallery</a></li>	-->
					<li id="Profile-chinese" onclick="func(&#39;Profile-chinese&#39;)"><a href="./Profile-chinese.html" class="scroll">中文简介</a></li>
					<li id="Contact" onclick="func(&#39;Contact&#39;)"><a href="./Contact.html" class="scroll">Contact</a></li>
				</ul>
			</div>
			<div class="clear"> </div>
			<!---End-top-nav--->
	     </div>
	</div>
	
	<script> 
	var lastname = 'Me';
	function func(name){
		var div = document.getElementById(name); 
		div.className = 'active'; 
		div = document.getElementById(lastname); 
		div.className = ''; 
		lastname = name;
	}
	function coming_soon()
	{
		alert("Coming soon.");
	}
	</script> 
	
		<!---End-wrap--->
	<div class="content">
		<div class="grid3">
			<div class="grid3-content">
		<h3><b>Selected Publications and Patents:</b></h3>
	
                

	
	<!--COPYRIGHT: The copyright of the following materials belongs to corresponding publishers. 
They are provided only for research and educational use that does not conflict to the interests of the publishers.
-->
	  </ul>
  <hr />			
<!-- 			
<h4><b>Pre-print:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
				

  </ul>
  <hr />-->
				

<h4><b>Books:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">

<li > <strong>Runmin Cong</strong>, Hao Chen, Hongyuan Zhu, and Huazhu Fu, 
  <a href="https://link.springer.com/chapter/10.1007/978-3-030-28603-3_10" target="_blank"><font color="#0000FF">Foreground detection and segmentation in RGB-D images</font></a>,
  in <ud2><i>Paul Rosin, Yukun Lai, Yonghuai Liu, and Ling Shao, RGB-D Image Analysis and Processing</i>, Springer</ud2>, ISBN 978-3-030-28602-6, Dec. 2019. (Book Chapter) <br></li>

<li > Chongyi Li, Huazhu Fu, Miao Yang, <strong>Runmin Cong</strong>, and Chunle Guo, 
  <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811218842_0010" target="_blank"><font color="#0000FF">Deep retinal image non-uniform illumination removal</font></a>,
  in <ud2><i>Zhenghua Chen, Min Wu, and Xiaoli Li, Generalization with Deep Learning: For Improvement on Sensing Capability</i>, World Scientific</ud2>, ISBN 978-981-121-883-5, Apr. 2021. (Book Chapter) <br></li>
  	
</ul>
	<hr />	


<h4><b>2026:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">  

<li> <strong>Runmin Cong</strong>, Anpeng Wang, Bin Wan, Cong Zhang, Xiaofei Zhou, and Wei Zhang, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Divide-and-conquer decoupled network for cross-domain few-shot segmentation</font></a>, 
  <ud2>The Fortieth AAAI Conference on Artificial Intelligence (AAAI)</ud2>, 2026. <strong><font color="#00CC33"><i>Oral</i></font></strong><br>
</li>
  

<li> Zhiyang Chen, Chen Zhang, Hao Fang, and <strong>Runmin Cong*</strong>, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Empowering DINO representations for underwater instance segmentation via aligner and prompter</font></a>, 
  <ud2>The Fortieth AAAI Conference on Artificial Intelligence (AAAI)</ud2>, 2026. <strong><font color="#00CC33"><i>Oral</i></font></strong> (* corresponding author) <br>
</li>

	
<li> Jia Lin, Xiaofei Zhou, Jiyuan Liu, <strong>Runmin Cong</strong>, Guodao Zhang, Zhi Liu, and Jiyong Zhang, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">SAM-DAQ: Segment anything model with depth-guided adaptive queries for RGB-D video salient object detection</font></a>, 
  <ud2>The Fortieth AAAI Conference on Artificial Intelligence (AAAI)</ud2>, 2026. <br>
</li>
  
		
		
		</ul>
			<hr />

<h4><b>2025:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">	
	  
<li> Jinpeng Chen, <strong>Runmin Cong*</strong>, Yuxuan Luo, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10904177" target="_blank"><font color="#0000FF">Replay without saving: Prototype derivation and distribution rebalance for class-incremental semantic segmentation</font></a>,
  <ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence</ud2>, vol. 47, no. 6, pp. 4699-4716, 2025.  (* corresponding author) 
  <!--<a href="https://github.com/zihaodong/UAFR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 

<li> Peisong Wen, Qianqian Xu, <strong>Runmin Cong</strong>, and Qingming Huang,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Semantic concentration for self-supervised dense representation learning</font></a>,
  <ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence</ud2>, 2025. 
  <!--<a href="https://github.com/zihaodong/UAFR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 

<li> Feng Li, <strong>Runmin Cong*</strong>, Jingjing Wu, Huihui Bai, Meng Wang, and Yao Zhao,
  <a href="https://link.springer.com/article/10.1007/s11263-024-02147-y" target="_blank"><font color="#0000FF">SRConvNet: A Transformer-style ConvNet for lightweight image super-resolution</font></a>,
  <ud2>International Journal of Computer Vision</ud2>, vol. 133, pp. 173-189, 2025.  (* corresponding author) 
  <a href="https://github.com/lifengcs/SRConvNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <!--<font color="#999999">[Code]</font>-->
</li>

<li> <strong>Runmin Cong</strong>, Rongxin Liao, Feng Li, Ronghui Sheng, Huihui Bai, Renjie Wan, Sam Kwong, and Wei Zhang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11033676" target="_blank"><font color="#0000FF">Reference-based iterative interaction with P^2-matching for stereo image super-resolution</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 34, pp. 3779-3789, 2025.
</li> 

<li> Hao Fang, <strong>Runmin Cong*</strong>, Xiankai Lu, Xiaofei Zhou, Sam Kwong, and Wei Zhang,
  <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_Decoupled_Motion_Expression_Video_Segmentation_CVPR_2025_paper.pdf" target="_blank"><font color="#0000FF">Decoupled motion expression video segmentation</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, 2025. (* corresponding author) 
  <!--<a href="https://github.com/zihaodong/UAFR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 
	  
<li> <strong>Runmin Cong</strong>, Ning Yang, Hongyu Liu, Dingwen Zhang, Qingming Huang, Sam Kwong, and Wei Zhang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10855555" target="_blank"><font color="#0000FF">TRNet: Two-tier recursion network for co-salient object detection</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, 2025.
  <!--<a href="https://github.com/zihaodong/UAFR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>

<li> <strong>Runmin Cong</strong>, Zongji Yu, Hao Fang, Haoyan Sun, and Sam Kwong,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">UIS-Mamba: Exploring Mamba for underwater instance segmentation via dynamic tree scan and hidden state weaken</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, 2025.  
  <!--<a href="https://github.com/rmcong/PICR-Net_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>

<li> Hang Xiong, <strong>Runmin Cong*</strong>, Jinpeng Chen, Chen Zhang, Feng Li, Huihui Bai, and Sam Kwong,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">MM-Prompt: Multi-modality and multi-granularity prompts for few-shot segmentation</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, 2025. (* corresponding author)
  <!--<a href="https://github.com/rmcong/PICR-Net_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>

<li> Hua Li, Gaowei Lin, Zhiyuan Li, Sam Kwong, and <strong>Runmin Cong*</strong>,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">FSCDiff: Frequency-spatial entangled conditional diffusion model for underwater salient object detection</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, 2025.  (* corresponding author)
  <!--<a href="https://github.com/rmcong/PICR-Net_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>  

<li> Jinpeng Chen†, <strong>Runmin Cong†</strong>, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">SEFE: Superficial and essential forgetting eliminator for multimodal continual instruction tuning</font></a>,
  <ud2>Forty-second International Conference on Machine Learning (ICML)</ud2>, 2025. （† equal contribution）
  <!--<a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li> Qi Qin, <strong>Runmin Cong*</strong>, Gen Zhan, Yiting Liao, and Sam Kwong,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">From sight to insight: Unleashing eye-tracking in weakly supervised video salient object detection</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, 2025.  (* corresponding author) 
</li> 

<li > Yuxuan Luo, Jinpeng Chen, <strong>Runmin Cong*</strong>, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10937737" target="_blank"><font color="#0000FF">Concept-level semantic transfer and context-level distribution modeling for few-shot segmentation</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 35, no. 9, pp. 9190-9204, 2025.  (* corresponding author) 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li > Wujie Zhou, Zijun Ju, <strong>Runmin Cong*</strong>, and Weiqing Yan,
  <a href="https://ieeexplore.ieee.org/document/11192489" target="_blank"><font color="#0000FF">RCNet: Dual-network resonance collaboration via mutual learning for RGB-D road defect detection</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, 2025.  (* corresponding author)
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li > Yuxuan Luo, Jinpeng Chen, <strong>Runmin Cong</strong>, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320325X00054/1-s2.0-S0031320325002730/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIAwYtalTK%2BBMX3bFebhoi6pfickjc8vgi0EQVo29B%2BsSAiEAgdwSJ8QL67cZPFNSwWMhjkE5WKLp5tJHzlhlyyht2H0qvAUI%2Bv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNNhbDzi%2BIReLO5b7iqQBdTFCnmPYIqL7NAIVWN7R93bWOdokvKozU6lP39MPbcWeCUdlXIrFuG2%2F6rT2HvKO01UnC%2BiBFUO%2BTd4UmZZKavf%2BZR6QFs5MBI40Lt2AqEuvZ33QQnR%2B9zRYkSuVE0HjIDFESZWhOmENXtSU7HG7RGVvoLsEzUkIkJufn18opZF2h0v%2B9i0AtRXh2jSzLgKQFl2QwzLFgPpoXFJMq%2FuVgBI4MLuSuuSmw%2FLYz%2Fz2LkKgpFjxxheTt%2FAf%2FRi24fk4p1ar17MkfWA2Ua9NJ6JXKx8pgOrAC%2BFHfJIsSo0hYySXNF562CoSlHklT0ETFmxeMoFhAs%2BJp%2FDQAzi1KT78ASfu6ZXHYGZruxcW%2BAT1TefZcObPncu0cE5usjj5KzuZVdGxs6awMAj%2BqzWDJZVzDSy%2BQsar8iAzSS%2BDKl4hxHzOWR7tChJsYjZQnRuDh3wJfEt9YAfex0VWtJrt5e%2Bdyfee6vWXE9BBHe6LnrcABy52bH1MUGqPyav4VWYrBSjKDddAeXpatSmSyx6cZg6ZloN2KXhAXcmnkZvDKfkDXyiRXFLtqZc3taWr3Wd3cG6M1sVWJtJN3agWcHVFR7KaykmgGSyHuJY4DAKOvTcNsFAeMH5znHXIPiN6vAn1bOVqT%2BRQ225aoeV93WgYiEdHxjvrF8sVSYUGgdnfcd12KhVkDiO3FkJ9JISOCOO%2FpxlTq8rH%2BuVcyy9rwHkMQOVHpV12JhF%2BdTA5QyrLCmGcmXfvN7frPf9fCYFkX%2BsMWm78DFt9D0DFQO6UT2waJsE1OKl0OEGKcPKZtKw0pwx9iH6EEavT50Wws1ktQ10S6OHJaga7XIGcyBuxF5eNgjcFR%2FUrL4qPsctGGwniqmIuhCFML7evL8GOrEBTIjFLQ6NGdn7DaDA0gcIU67e4dGQSeCPDI3eJwc%2BYVFSSdC9no6RusnCts1P9Oiail%2FcsLj3qQY28AcMoHAuFaT4yxlMo%2BVsW1ZLwLoUfe8os5bqqU1yCaBKrHByfUUPJe5Uuo%2BmWJQyPBGtK3oJ4CTU863spzeHtNDEuRmswADWBGTfUmwZ8YiGiB2bMq%2BcJQzN0kb1i%2B15OWgvATxZ0p%2FiEhFMas36RJbsaEWFON5j&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250404T014316Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUQ5WIIHZ%2F20250404%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f16efed2c23089ce042cbea340cad42033b2cf00a59c4fe3ccf06617bb592fe5&hash=3dc88eb7f413c237a8e51d117d57e5d0f13a8270c1ca3562ac78f938b03d9207&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320325002730&tid=spdf-f1de2adf-f8e9-4fbd-a12f-d9ce58ff6d80&sid=0aa2cf9745ff964a559abd04f625654cdcf9gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=190856500105510b515956&rr=92ad1d2698e0dd5d&cc=cn" target="_blank"><font color="#0000FF">Trace back and go ahead: Completing partial annotation for continual semantic segmentation</font></a>,
  <ud2>Pattern Recognition</ud2>, Vol. 165, artile no. 111613, 2025.  
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li > Lidong Xie, <strong>Runmin Cong</strong>,  Ju Dai, Wenhan Yang, Junjun Pan, and Hao Wu,
  <a href="https://www.sciencedirect.com/science/article/pii/S0031320325010210?dgcid=coauthor" target="_blank"><font color="#0000FF">CTNet: Color transformation network for low-light image enhancement</font></a>,
  <ud2>Pattern Recognition</ud2>, Vol. 172, artile no. 112360, 2026.  
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Xiufeng Huang, Ka Chun Cheung, <strong>Runmin Cong</strong>, Simon See, and Renjie Wan,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Stereo-GS: Multi-view stereo vision model for generalizable 3D Gaussian splatting reconstruction</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, 2025.  
  <!--<a href="https://github.com/rmcong/PICR-Net_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>


<li> Zifeng Qiu, Hongyu Liu, Hang Xiong, Chengliang Di, Hao Fang, and <strong>Runmin Cong</strong>,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Shape embedding and knowledge mining network for generalized few-shot remote sensing segmentation</font></a>,
  <ud2>IEEE Geoscience and Remote Sensing Letters</ud2>, 2025.  
  <!--<a href="https://github.com/lifengcs/SRConvNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li>
	  
<li> Liuxin Bao, Xiaofei Zhou, Bolun Zheng, <strong>Runmin Cong</strong>, Haibing Yin, Jiyong Zhang, and Chenggang Yan,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">IFENet: Interaction, fusion, and enhancement network for V-D-T salient object detection</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 34, pp. 483-494, 2025.
</li> 

<li> Huiyang Wu, Qiuping Jiang, Zongwei Wu, <strong>Runmin Cong</strong>, Cedric Demonceaux, and Yi Yang,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">High-resolution underwater creature segmentation</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, 2025.
</li> 

<li>  Qiuping Jiang, Jinguang Cheng, Zongwei Wu, <strong>Runmin Cong</strong>, and Radu Timofte, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">High-precision dichotomous image segmentation with frequency and scale awareness</font></a>,
  <ud2>IEEE Transactions on Neural Networks and Learning Systems</ud2>, 2025. <br>
  <!--<a href="https://rmcong.github.io/proj_MPFRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/MPFRNet_TNNLS2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->  
</li> 	 

<li> Mingzhu Xu, Sen Wang, Yupeng Hu, Haoyu Tang, <strong>Runmin Cong*</strong>, and Liqiang Nie,
  <a href="https://ieeexplore.ieee.org/document/11163514" target="_blank"><font color="#0000FF">Cross-Model nested fusion network for salient object detection in optical remote sensing images</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 55, no. 11, pp. 5332-5345, 2025. 
  <!--<a href="https://rmcong.github.io/proj_MPFRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/MPFRNet_TNNLS2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->  
</li> 

<li> Jianhui Jin, Qiuping Jiang, Qingyuan Wu, Binwei Xu, and <strong>Runmin Cong</strong>,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10744585" target="_blank"><font color="#0000FF">Underwater salient object detection via dual-stage self-paced learning and depth emphasis</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 35, no. 3, pp. 2147-2160, 2025. <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong>
  <a href="https://github.com/NIT-JJH/SPDE"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li > Yixuan Wu, Feng Li, <strong>Runmin Cong</strong>, Huihui Bai, Weisi Lin, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9925720" target="_blank"><font color="#0000FF">Bridging component learning with degradation modelling for blind image super-resolution</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, 2025. <strong><font color="#00CC33"><i>Popular Documents in TMM (2022/10)</i></font></strong>
  <a href="https://github.com/Arcananana/CDCN"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	

<li > Lingzhi He, Yakun Chang, <strong>Runmin Cong</strong>, Hongyu Liu, Shujuan Huang, Renshuai Tao, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10891560" target="_blank"><font color="#0000FF">Rethinking depth guided reflection removal</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, 2025. 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Anqi Li, Feng Li, Yuxi Liu, <strong>Runmin Cong</strong>, Yao Zhao, and Huihui Bai,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Once-for-All: Controllable generative image compression with dynamic granularity adaption</font></a>,
  <ud2>The Thirteenth International Conference on Learning Representations (ICLR)</ud2>, 2025. 
  <!--<a href="https://github.com/lifengcs/SRConvNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li > Xiaofei Zhou, Ming Peng, Qiuping Jiang, <strong>Runmin Cong</strong>, Jiyong Wang, and Yun Chen,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10890916" target="_blank"><font color="#0000FF">CA-Net: Cascaded adaptive network for underwater image enhancement</font></a>,
  <ud2>IEEE Journal of Oceanic Engineering</ud2>, 2025. 
  <a href="https://github.com/hdupm/CA-Net"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li > Zeyong Zhang, Fuxiang Feng, Zifeng Qiu, and <strong>Runmin Cong</strong>,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10890916" target="_blank"><font color="#0000FF">Exploring the potential of scene features for UAV object detection</font></a>,
  <ud2>IEEE Intelligent Systems</ud2>, 2025. 
  <a href="https://github.com/hdupm/CA-Net"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Shuxian Ma, Zihao Dong, <strong>Runmin Cong</strong>, Sam Kwong, and Xiuli Shao,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Proto-FG3D: Prototype-based interpretable fine-grained 3D shape classification</font></a>, 
  <ud2>The Thirty Sixth British Machine Vision Conference</ud2>, 2025.
</li> 

<li> Yibo Chen, Zihao Dong, Jinping Li, <strong>Runmin Cong</strong>, and Xiuli Shao,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Diffusion-guided domain-adaptive segmentation with structure-aware learning via retinal image noise conditioning</font></a>,
  <ud2>International Joint Conference on Neural Networks (IJCNN)</ud2>, 2025. 
  <!--<a href="https://github.com/lifengcs/SRConvNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li> <strong>丛润民</strong>, 孙豪言, 罗宇轩, 方豪,
  <a href="https://hkxb.buaa.edu.cn/CN/10.7527/S1000-6893.2025.31694" target="_blank"><font color="#0000FF">基于类关系挖掘的遥感图像广义小样本分割方法</font></a>, 
  <ud2>航空学报</ud2>, 46(24):631694, 2025. 
</li>

<li> 方岩, 魏云超, <strong>丛润民</strong>, 左旺孟, 赵耀,
  <a href="https://www.ejournal.org.cn/thesisDetails#10.12263/DZXB.20240750&lang=zh" target="_blank"><font color="#0000FF">连续学习方法与其在视觉任务中的应用</font></a>, 
  <ud2>电子学报</ud2>, 2025. 
</li>

	  	  </ul>
  <hr />
	

				
<h4><b>2024:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">	

<li> <strong>Runmin Cong</strong>, Chunlei Wu, Xibin Song, Wei Zhang, Sam Kwong, Hongdong Li, and Pan Ji,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10696933" target="_blank"><font color="#0000FF">SRNSD: Structure-regularized night-time self-supervised monocular depth estimation for outdoor scenes</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 33, pp. 5538-5550, 2024. 
  <!-- <a href="https://github.com/rmcong/QPENet_TMM24"  target="_blank"><font color="#FF5151">[Code]</font></a> -->  
</li> 
	  
<li >  <strong>Runmin Cong</strong>, Ke Huang, Jianjun Lei, Yao Zhao, Qingming Huang, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10012430" target="_blank"><font color="#0000FF">Multi-projection fusion and refinement network for salient object detection in 360° omnidirectional image</font></a>,
  <ud2>IEEE Transactions on Neural Networks and Learning Systems</ud2>, vol. 35, no. 7, pp. 9495-9507, 2024. <br>
  <!--<a href="https://rmcong.github.io/proj_MPFRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/MPFRNet_TNNLS2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->  
</li> 	 	  

<li> <strong>Runmin Cong</strong>, Hang Xiong, Jinpeng Chen, Wei Zhang, Qingming Huang, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10388457" target="_blank"><font color="#0000FF">Query-guided prototype evolution network for few-shot segmentation</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 26, pp. 6501-6512, 2024. 
  <a href="https://github.com/rmcong/QPENet_TMM24"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li> 

<li> <strong>Runmin Cong</strong>, Ronghui Sheng, Hao Wu, Yulan Guo, Yunchao Wei, Wangmeng Zuo, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10478547" target="_blank"><font color="#0000FF">Learning hierarchical color guidance for depth map super-resolution</font></a>, 
  <ud2>IEEE Transactions on Instrumentation and Measurement</ud2>, vol. 74, pp. 1-13, 2024.
<a href="https://github.com/rmcong/HCGNet_TIM2024"  target="_blank"><font color="#FF5151">[Code]</font></a>
 </li>

<li> Hongyu Liu, <strong>Runmin Cong*</strong>, Hua Li, Qianqian Xu, Qingming Huang, and Wei Zhang,
  <a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24l/liu24l.pdf" target="_blank"><font color="#0000FF">ESNet: Evolution and succession network for high-resolution salient object detection</font></a>,
  <ud2>Forty-first International Conference on Machine Learning (ICML)</ud2>, pp. 30892-30907, 2024. (* corresponding author) 
  <!--<a href="https://github.com/lifengcs/SRConvNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 
	  
<li> Jinpeng Chen, <strong>Runmin Cong*</strong>, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10314036" target="_blank"><font color="#0000FF">KepSalinst: Using peripheral points to delineate salient instances</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 54, no. 6, pp. 3392-3405, 2024. (* corresponding author) <strong><font color="#00CC33"><i>Popular Documents in TCyb (2023/01-2023/05)</i></font></strong>
 <a href="https://github.com/jinpeng0528/KepSalinst"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
   <!--<font color="#999999">[Code]</font>-->
</li> 	

<li> Jinpeng Chen, <strong>Runmin Cong*</strong>, Yuxuan Luo, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Strike a balance in continual panoptic segmentation</font></a>,
  <ud2>European Conference on Computer Vision</ud2>, 2024. (* corresponding author)
</li> 	

<li> Yuxuan Luo, <strong>Runmin Cong*</strong>, Xialei Liu, Horace Ho Shing Ip, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10557156" target="_blank"><font color="#0000FF">Modeling inner- and cross-task contrastive relations for continual image classification</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 26, pp. 10842-10853, 2024. (* corresponding author)
</li> 

<li> Xiaofei Zhou, Zhicong Wu, and <strong>Runmin Cong*</strong>,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10417767" target="_blank"><font color="#0000FF">Decoupling and integration network for camouflaged object detection</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 26, pp. 7114-7129, 2024. (* corresponding author, <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong>) 
  <a href="https://github.com/zhicong01/DINet"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 
	  
<li> Hua Li, Junyan Liang, Ruiqi Wu, <strong>Runmin Cong*</strong>, Wenhui Wu, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10098144" target="_blank"><font color="#0000FF">Stereo superpixel segmentation via decoupled dynamic spatial-embedding fusion network</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 26, pp. 367-378, 2024. (* corresponding author) 
  <!--<a href="https://github.com/Arcananana/DSSR"  target="_blank"><font color="#FF5151">[Code]</font></a>  <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li> Wujie Zhou, Penghan Yang, Yuanyuan Liu, <strong>Runmin Cong*</strong>, and Qiuping Jiang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10374103" target="_blank"><font color="#0000FF">Remote sensing image scene classification via graph template enhancement and supplementation network with dual-teacher knowledge distillation</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 62, pp. 1-13, 2024.  (* corresponding author) 
  <a href="https://github.com/MAXHAN22/GTESNet"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li>

<li> Shijie Lian, Ziyi Zhang, Hua Li, Wenjie Li, Laurence Tianruo Yang, Sam Kwong, and <strong>Runmin Cong</strong>,
  <a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/lian24c/lian24c.pdf" target="_blank"><font color="#0000FF">Diving into underwater: Segment anything model guided underwater salient instance segmentation and a large-scale dataset</font></a>,
  <ud2>Forty-first International Conference on Machine Learning (ICML)</ud2>, pp. 29545-29559, 2024.
  <!--<a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li> Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, <strong>Runmin Cong</strong>, Xiaochun Cao, and Qingming Huang,
  <a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24bx/li24bx.pdf" target="_blank"><font color="#0000FF">Size-invariance matters: Rethinking metrics and losses for imbalanced multi-object salient object detection</font></a>,
  <ud2>Forty-first International Conference on Machine Learning (ICML)</ud2>, pp. 28989-29021, 2024. 
  <!--<a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 

<li >  Qian Zhang, Lin Zhang, Ran Song, <strong>Runmin Cong</strong>, Yonghuai Liu, and Wei Zhang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10630657" target="_blank"><font color="#0000FF">Learning common semantics via optimal transport for contrastive multi-view clustering</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 33, pp. 4501-4515, 2024. <br>
  <!--<a href="https://rmcong.github.io/proj_MPFRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/MPFRNet_TNNLS2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->  
</li> 	
	  
<li> Lingzhi He, Feng Li, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10309937" target="_blank"><font color="#0000FF">Reflection intensity guided single image reflection removal and transmission recovery</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 26, pp. 5026-5039, 2024. 
  <!--<a href="https://github.com/Arcananana/DSSR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 

<li> Zifeng Qiu, Tianyu Gong, Zichao Liang, Taoyi Chen, <strong>Runmin Cong</strong>, Huihui Bai, and Yao Zhao, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10508405" target="_blank"><font color="#0000FF">Perception-oriented UAV image dehazing based on super-pixel scene prior</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 62, article No. 5913519, 2024. <br>
  </li>

<li> Zihao Dong, Zizhen Liu, <strong>Runmin Cong</strong>, Tiyu Fang, Xiuli Shao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10535315" target="_blank"><font color="#0000FF">UAFer: A unified model for class-agnostic binary segmentation with uncertainty-aware feature reassembly</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 34, no. 10, pp. 9836-9851, 2024.
  <a href="https://github.com/zihaodong/UAFR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Dongyi Zhang, Feng Li, Man Liu, <strong>Runmin Cong</strong>, Huihui Bai, Meng Wang, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10226331" target="_blank"><font color="#0000FF">Exploring resolution fields for scalable image compression with uncertainty guidance</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 34, no. 4, pp. 2934-2948, 2024. 
  <a href="https://github.com/JGIroro/RPNSIC"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Jinyu Yang, Mingqi Gao, <strong>Runmin Cong</strong>, Chengjie Wang, Feng Zheng, and Ales Leonardis,
  <a href="https://ieeexplore.ieee.org/document/10368006" target="_blank"><font color="#0000FF">Unveiling the power of visible-thermal video object segmentation</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 34, no. 7, pp. 5376-5388, 2024. 
  <a href="https://github.com/JGIroro/RPNSIC"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Guanghui Yue, Jie Gao, <strong>Runmin Cong</strong>, Tianwei Zhou, Leida Li, and Tianfu Wang，
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10274690" target="_blank"><font color="#0000FF">Deep pyramid network for low-light endoscopic image enhancement</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 34, no. 5, pp. 3834-3845, 2024. 
  <!--<a href="https://github.com/JGIroro/RPNSIC"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 



<li> Zhipeng Wang, Dan Ma, Guanghui Yue, Beichen Li, <strong>Runmin Cong</strong>, and Zhiqiang Wu, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10374103" target="_blank"><font color="#0000FF">Self-supervised hyperspectral anomaly detection based on finite spatial-wise attention</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 64, pp. 1-18, 2024. 
</li>

<li> Weiqing Yan, Yanshun Chen, Wujie Zhou, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10374103" target="_blank"><font color="#0000FF">MVoxTi-DNeRF: Explicit multi-scale voxel interpolation and temporal encoding network for efficient dynamic neural radiance field</font></a>, 
  <ud2>IEEE Transactions on Automation Science and Engineering</ud2>, 2024. In press.
</li>

<li> Feng Li, Yixuan Wu, Anqi Li, Huihui Bai, <strong>Runmin Cong</strong>, and Yao Zhao, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Enhanced video super-resolution network towards compressed data</font></a>, 
  <ud2>ACM Transactions on Multimedia Computing Communications and Applications</ud2>, vol. 20, no. 7, article no. 202, 2024. 
</li>

<li> Feng Li, Yixuan Wu, Zichao Liang, <strong>Runmin Cong</strong>, Huihui Bai, Yao Zhao, and Meng Wang, 
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">BlindDiff: Empowering degradation modelling in diffusion models for blind image super-resolution</font></a>, 
  <ud2>Science China-Information Science</ud2>, 2024. 
</li>

<li> Jieru Yao, Longfei Han, Guangyu Guo, Zhaohui Zheng, <strong>Runmin Cong</strong>, Xiankai Huang, Jin Ding, Kaihui Yang, Dingwen Zhang, and Junwei Han,
  <a href="https://www.sciencedirect.com/science/article/pii/S0893608023007001?dgcid=coauthor" target="_blank"><font color="#0000FF">Position-based anchor optimization for point supervised dense nuclei detection</font></a>,
  <ud2>Neural Networks</ud2>, vol. 171, pp. 159-170, 2024. 
  <a href="https://github.com/NucleiDet/DenseNucleiDet"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li> 

<li> Yinsong Xu, Yulong Ding, Jie Jiang, <strong>Runmin Cong</strong>, Xuefeng Zhang, Shiqi Wang, Sam Kwong, and Shuang-Hua Yang,
  <a href="https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231224X00399/1-s2.0-S0925231224011998/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD7sD0mxoD4rr5vUeDUSC%2FTM1jfsJud1JbnAtFW%2FqNWAwIgfPkM28924gQaaGgXjo8fTmK%2BCYkCbl6WgUThIK1bnEYqswUIFxAFGgwwNTkwMDM1NDY4NjUiDO9y2ejHlzviV3yYvyqQBSxx67UXIiKih%2Fahgc48atTH467ij8giOs%2FsgVoQlPJpaIpS2AEKnugh%2Bs16wxQChuJjp61ekhh6SPWI8wg67y43p9qFLZ044VmQKg3%2BpGZOy6dlk15gBaQ%2FZyEuApACDUaN8zGl37%2Bu%2FaltuZqtsNAG5sDSZXQDP8CTf9nToIB8Hx5H7vSN4IjOP4X3Pns%2BtpBjVylQcho0zRhmSWLBLHyEvnDJ4BkNdZIHfpB29SqjqqqcxBK%2BM1%2FXhFceCudksj6VgyPWXKSyWqL8KugdH8XY3Fyi317eEBaN0vPPQp9KLBYeuODuijfDITQ%2FPzVxUL%2Fs2l1XzUA3cipYpEutylyH3nbCEVhFv%2BMTlnmrmOjE4ZrzxRE3bLVZGz%2FQDhigOh%2Bw2E626Yc%2FFvWcP5q2b1MMZlSvDuBDssAzvs2IkMSx9GOOtrrIbLTBIOPitVTc3Xpf9PiB6MvxFYC9j7ZuW4Tj3L1hcJCc2WSyzcOe7Ajag9%2FHVRZiJfpUaMvLpYsDxmoh9FFHesW9C5GvdYXVXF8tNbYpQURV56GAxShQEnwgSVDoboEHH3tX0MtfptT%2BlMrrX98ZEkZw8x2oF96xNg2FEk21eycHJR9Vs4KouM0kboKtYNtee1ssjRUpGbAtcnGz89ecX66wz1Euh%2BucErPEsPG7na4wjlERVdOS75Fp0sSFtPtPem43YqAiRoGnQvggka3AVEckBSlnxZ5HR4paQ3tprTUVNRislgka7CpQPR71GZC6JgMiYG%2Bv7UWfwnSKsnBe9PU6b30vHI0R%2F4tKmFEIfuQhv9%2BaLRj0TbdEQmUdqBvDyN1kByCc%2BMuEZv2OPGv5V2mvy3QhppcEaEOvh3chOU4LWYSSdXmNj0alMO25irgGOrEBrfqqZgLoKo55bFwWI%2BF2udINLVRnsQU5ZJ6pVInnkXeWVrqGUPa2y7DDFgn4Lk16bRRn%2BZ9yZ6Opl3Q0SkIaoHVKMrKmGGL%2BTEOq9TisHUAg79GE7RP6w7jGSSYZgYgiBNflJoKYJXaMqBuSdCHhEL2YvxGkL%2FcECrl7Kepz%2BvGLfOPGNq08s09Oh%2BaIsY5BgFOF44WLhlMLiyrfPvbzUnRnhYAmis4x7gFQdokIh2Hg&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241006T145221Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWSP5HYJU%2F20241006%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b43c7cc8c46d9e5b4605cc0a67a3f4aaaa321938b982c9c4f75233929104ff93&hash=bf2659c2ed9d77cfc9ae2bc95211261e334e731d52ff2889819f0f175be28e44&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0925231224011998&tid=spdf-08008829-f090-49cc-b29c-4fdb7a25d14f&sid=289c331f325d96414688dd20bec96cc8acdbgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=09015d000259575b560408&rr=8ce6798affbd3e4f&cc=sg" target="_blank"><font color="#0000FF">Skip-patching spatial-temporal discrepancy-based anomaly detection on multivariate time series</font></a>,
  <ud2>Neurocomputing</ud2>, vol. 609, article no. 128428, 2024. 
</li> 

<li> Pengwei Dong, Bo Wang, <strong>Runmin Cong</strong>, Hai-Han Sun, and Chongyi Li,
  <a href="https://www.sciencedirect.com/science/article/pii/S1077314223002977?dgcid=coauthor" target="_blank"><font color="#0000FF">Transformer with large convolution kernel decoder network for salient object detection in optical remote sensing images</font></a>,
  <ud2>Computer Vision and Image Understanding</ud2>, vol. 240, article no. 103917, 2024. </a> 
</li> 

	  
	  </ul>
  <hr />

				
<h4><b>2023:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">	

<li> <strong>Runmin Cong</strong>, Wenyu Yang, Wei Zhang, Chongyi Li, Chun-Le Guo, Qingming Huang, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10155564" target="_blank"><font color="#0000FF">PUGAN: Physical model-guided underwater image enhancement using GAN with dual-discriminators</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 32, pp. 4472-4485, 2023.  <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2023/08)</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_PUGAN.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/PUGAN_TIP2023"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
  <!--<a href="https://www.bilibili.com/video/BV1n24y117xW/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>-->
	  
	  
<li> <strong>Runmin Cong</strong>, Ning Yang, Chongyi Li, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9837785" target="_blank"><font color="#0000FF">Global-and-local collaborative learning for co-salient object detection</font></a>, 
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 53, no. 3, pp. 1920-1931, 2023. <br>
  <a href="https://rmcong.github.io/proj_GLNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/GLNet_TCYB2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
<a href="https://www.bilibili.com/video/BV1FP411M74q/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>
	  
<li> <strong>Runmin Cong</strong>, Kepu Zhang, Chen Zhang, Feng Zheng, Yao Zhao, Qingming Huang, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9926193" target="_blank"><font color="#0000FF">Does Thermal really always matter for RGB-T salient object detection?</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 25, pp. 6971-6982, 2023. <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_TNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/TNet_TMM2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
 <a href="https://www.bilibili.com/video/BV1Jg411q7s9/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li>
	  
<li> <strong>Runmin Cong</strong>, Qi Qin, Chen Zhang, Qiuping Jiang, Shiqi Wang, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9881551" target="_blank"><font color="#0000FF">A weakly supervised learning framework for salient object detection via hybrid labels</font></a>, 
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 33, no. 2, pp. 534-548, 2023. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCSVT (2023/02)</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_Hybrid-Label-SOD.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Hybrid-Label-SOD_TCSVT2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>	  
	  
<li> <strong>Runmin Cong</strong>, Weiyu Song, Jianjun Lei, Guanghui Yue, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9955382" target="_blank"><font color="#0000FF">PSNet: Parallel symmetric network for video salient object detection</font></a>, 
  <ud2>IEEE Transactions on Emerging Topics in Computational Intelligence</ud2>, vol. 7, no. 2, pp. 402-414, 2023. <strong><font color="#00CC33"><i>Popular Documents in TETCI (2023/03-2023/05; 2023/07-2023/08, 2023/12)</i></font></strong><br>
  <!--<a href="https://rmcong.github.io/proj_PSNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>-->
  <a href="https://github.com/rmcong/PSNet_TETCI2022"  target="_blank"><font color="#FF5151">[Results]</font></a>  </li>

<li> <strong>Runmin Cong</strong>, Hongyu Liu, Chen Zhang, Wei Zhang, Feng Zheng, Ran Song, and Sam Kwong,
  <a href="https://dl.acm.org/doi/10.1145/3581783.3611982" target="_blank"><font color="#0000FF">Point-aware interaction and CNN-induced refinement network for RGB-D salient object detection</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, pp. 406-416, 2023.  <strong><font color="#00CC33"><i>Oral</i></font></strong>
  <a href="https://github.com/rmcong/PICR-Net_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> <strong>Runmin Cong</strong>, Mengyao Sun, Sanyi Zhang, Xiaofei Zhou, Wei Zhang, and Yao Zhao,
  <a href="https://dl.acm.org/doi/10.1145/3581783.3612083" target="_blank"><font color="#0000FF">Frequency perception network for camouflaged object detection</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, pp. 1179-1189, 2023. <strong><font color="#00CC33"><i>Oral</i></font></strong>
  <a href="https://github.com/rmcong/FPNet_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> <strong>Runmin Cong</strong>, Yuchen Guan, Jinpeng Chen, Wei Zhang, Yao Zhao, and Sam Kwong,
  <a href="https://dl.acm.org/doi/10.1145/3581783.3612482" target="_blank"><font color="#0000FF">SDDNet: Style-guided dual-layer disentanglement network for shadow detection</font></a>,
  <ud2>ACM Multimedia (ACM MM)</ud2>, pp. 1202-1211, 2023. <strong><font color="#00CC33"><i>Oral</i></font></strong>
  <a href="https://github.com/rmcong/SDDNet_ACMMM23"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Jinpeng Chen, <strong>Runmin Cong*</strong>, Yuxuan Luo, Horace Ho Shing Ip, and Sam Kwong, 
  <a href="https://openreview.net/pdf?id=Ct0zPIe3xs" target="_blank"><font color="#0000FF">Saving 100x storage: Prototype replay for reconstructing training sample distribution in class-incremental semantic segmentation</font></a>,
  <ud2>Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</ud2>, 2023. (CCF A, * corresponding author) 
  <a href="https://github.com/MonkeyKing0528/STAR"  target="_blank"><font color="#FF5151">[Code]</font></a> 	<br>
</li>
	  
<li> <strong>丛润民</strong>, 张晨, 徐迈, 刘鸿羽, 赵耀,
  <a href="http://www.jos.org.cn/jos/article/pdf/6700" target="_blank"><font color="#0000FF">深度学习时代下的RGB-D显著性目标检测研究进展</font></a>, 
  <ud2>软件学报</ud2>, 34(4): 1711-1731, 2023.  (CCF A) <br></li>	

<li > Shijie Lian, Hua Li, <strong>Runmin Cong*</strong>, Suqi Li, Wei Zhang, and Sam Kwong,
  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.pdf" target="_blank"><font color="#0000FF">WaterMask: Instance segmentation for underwater imagery</font></a>,
  <ud2>International Conference on Computer Vision (ICCV)</ud2>, pp. 1305-1315, 2023. (* corresponding author)
  <a href="https://github.com/LiamLian0727/WaterMask"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 

<li> Xiaofei Zhou, Kunye Shen, Li Weng, <strong>Runmin Cong*</strong>, Bolun Zheng, Jiyong Zhang, and Chenggang Yan,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9756846" target="_blank"><font color="#0000FF">Edge-guided recurrent positioning network for salient object detection in optical remote sensing images</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 53, no. 1, pp. 539-552, 2023. (* corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCyb (2023/01-2023/05)</i></font></strong>
  <a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <!--<font color="#999999">[Code]</font>-->
</li> 		  

	  
<li> Wujie Zhou, Fan Sun, Qiuping Jiang, <strong>Runmin Cong*</strong>, and Jenq-Neng Hwang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10127616" target="_blank"><font color="#0000FF">WaveNet: Wavelet network with knowledge distillation for RGB-T salient object detection</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 32, pp. 3027-3039, 2023. (* corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2023/06-2023/09, 2023/12)</i></font></strong>
  <a href="https://github.com/nowander/WaveNet"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li> 

 <li > Heyu Huang, <strong>Runmin Cong*</strong>, Lianhe Yang, Ling Du, Cong Wang, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3571744" target="_blank"><font color="#0000FF">Feedback chain network for hippocampus segmentation</font></a>,
  <ud2>ACM Transactions on Multimedia Computing Communications and Applications</ud2>, vol. 19, no. 3s, article 133, 2023. (* corresponding author)
  <!--<a href="https://github.com/zxforchid/ERPNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <font color="#999999">[Code]</font>-->
</li> 
	  
<li> Jie Wu^, <strong>Runmin Cong</strong>^, Leyuan Fang, Chunle Guo, Bob Zhang, and Pedram Ghamisi,
  <a href="http://scis.scichina.com/en/2023/119105.pdf" target="_blank"><font color="#0000FF">Unpaired remote sensing image super resolution with content-preserving weak supervision neural network</font></a>,
  <ud2>Science China Information Science</ud2>, vol. 66, no. 1, pp. 119105:1-119105:2, 2023. (CCF A, ^ equal contribution)
  <a href="https://mp.weixin.qq.com/s/rcibO0EA_l-2RWsnjlzdxA"  target="_blank"><font color="#FF5151">[中文导读]</font></a> 
</li>
	  
<!---<li> Chunjie Zhang, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Structure decomposition of visual-semantic correlations for image classification with varied levels of supervision</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, 2023. In Press. (CCF A)<br>
  <a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>
</li> --->
	  
<li > Feng Li, Yixuan Wu, Huihui Bai, Weisi Lin, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9721549" target="_blank"><font color="#0000FF">Learning detail-structure alternative optimization for blind super-resolution</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 25, pp. 2825-2838, 2023. 
  <a href="https://github.com/Arcananana/DSSR"  target="_blank"><font color="#FF5151">[Code]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	  
	  
<li>Guanghui Yue, Siying Li, <strong>Runmin Cong</strong>, Tianwei Zhou, Baiying Lei, and Tianfu Wang,
  <a href="https://ieeexplore.ieee.org/document/10058111" target="_blank"><font color="#0000FF">Attention-guided pyramid context network for polyp segmentation in colonoscopy images</font></a>, 
  <ud2>IEEE Transactions on Instrumentation and Measurement</ud2>, vol. 72, article no. 5008213, 2023. <strong><font color="#00CC33"><i>Popular Documents in TIM (2023/04-2023/09)</i></font></strong>
</li>	 

<li> Wujie Zhou, Bingying Wang, Weiqing Yan, Qiuping Jiang, and <strong>Runmin Cong</strong>,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">PENet-KD: Progressive enhancement network via knowledge distillation for rail surface defect detection</font></a>, 
  <ud2>IEEE Transactions on Instrumentation and Measurement</ud2>, vol. 72, article no. 5032811, 2023. <br>
</li>	 
	  
<li > Xiaoyu Zhang, Wei Gao, Ge Li, Qiuping Jiang, and <strong>Runmin Cong</strong>, 
  <a href="https://dl.acm.org/doi/pdf/10.1145/3532625" target="_blank"><font color="#0000FF">Image quality assessment driven reinforcement learning for mixed distorted image restoration</font></a>,
  <ud2>ACM Transactions on Multimedia Computing Communications and Applications</ud2>, vol. 19, no. 1s, article no. 42, 2023. <br>
</li> 
	  	
	  
<li > Tiantian Geng, Teng Wang, Jinming Duan, <strong>Runmin Cong</strong>, and Feng Zheng, 
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Dense-Localizing_Audio-Visual_Events_in_Untrimmed_Videos_A_Large-Scale_Benchmark_and_CVPR_2023_paper.pdf" target="_blank"><font color="#0000FF">Dense-localizing audio-visual events in untrimmed videos: A large-scale benchmark and baseline</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 22942-22951, 2023. (CCF A) 
	<a href="https://unav100.github.io"  target="_blank"><font color="#FF5151">[Code]</font></a><br>
</li> 

<li > Weiqing Yan, Yiqiu Sun, Wujie Zhou, Zhaowei Liu, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10304202" target="_blank"><font color="#0000FF">Deep video stabilization via robust homography estimation</font></a>,
  <ud2>IEEE Signal Processing Letters</ud2>, vol. 30, pp. 1602-1606, 2023. <br>
</li> 

	  
	  </ul>
  <hr />
			
			
			
			
	
<h4><b>2022:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
<li> <strong>Runmin Cong</strong>, Qinwei Lin, Chen Zhang, Chongyi Li, Xiaochun Cao, Qingming Huang, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9930882" target="_blank"><font color="#0000FF">CIR-Net: Cross-modality interaction and refinement for RGB-D salient object detection</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 31, pp. 6800-6815, 2022.  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_CIRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/CIRNet_TIP2022"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  
 <a href="https://www.bilibili.com/video/BV1n24y117xW/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>

<li> <strong>Runmin Cong</strong>, Yumo Zhang, Leyuan Fang, Jun Li, Yao Zhao, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592773" target="_blank"><font color="#0000FF">RRNet: Relational reasoning network with parallel multi-scale attention for salient object detection in optical remote sensing images</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 60, pp. 5613311, 2022.  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_RRNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/RRNet_TGRS2021"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>	
  <a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>
	  
<li> <strong>Runmin Cong</strong>, Haowei Yang, Qiuping Jiang, Wei Gao, Haisheng Li, Cong Wang, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9849697" target="_blank"><font color="#0000FF">BCS-Net: Boundary, context and semantic for automatic COVID-19 lung infection segmentation from CT images</font></a>, 
  <ud2>IEEE Transactions on Instrumentation and Measurement</ud2>, vol. 71, pp. 1-11, 2022. <br>
  <a href="https://github.com/rmcong/BCS-Net-TIM22"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>	 
	  
<li> <strong>Runmin Cong</strong>, Yumo Zhang, Ning Yang, Haisheng Li, Xueqi Zhang, Ruochen Li, Zewen Chen, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9882382" target="_blank"><font color="#0000FF">Boundary guided semantic learning for real-time COVID-19 lung infection segmentation system</font></a>, 
  <ud2>IEEE Transactions on Consumer Electronics</ud2>, vol. 68, no. 4, pp. 376-386, 2022. <strong><font color="#00CC33"><i>IEEE Chester W. Sall Memorial Award (Second Place), Popular Documents in TCE (2022/10-2023/04, 2023/12-2024/01)</i></font></strong><br>
  <a href="https://github.com/rmcong/BSNet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>  </li>
	  
	  
<li > Qiuping Jiang, Yudong Mao, <strong>Runmin Cong</strong>, Wenqi Ren, Chao Huang, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9757816" target="_blank"><font color="#0000FF">Unsupervised decomposition and correction network for low-light image enhancement</font></a>,
  <ud2>IEEE Transactions on Intelligent Transportation Systems</ud2>,  vol. 23, no. 10,  pp. 19440-19455, 2022.<br>
  <a href="https://github.com/myd945/UDCN"  target="_blank"><font color="#FF5151">[Code]</font></a> 
</li> 	

 
	  
 <li > Feng Li, Jia Qin, Huihui Bai, Weisi Lin, <strong>Runmin Cong</strong>, and Yao Zhao,
  <a href="https://ieeexplore.ieee.org/document/9827481" target="_blank"><font color="#0000FF">SRInpaintor: When super-resolution meets Transformer for image inpainting</font></a>,
  <ud2>IEEE Transactions on Computational Imaging</ud2>, vol. 8, pp. 743-758, 2022. <strong><font color="#00CC33"><i>Popular Documents in TCI (2022/08-2023/02)</i></font></strong><br>
  <!--<a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 	
	  
<li > Qiuping Jiang, Yuese Gu, Chongyi Li, <strong>Runmin Cong</strong>, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9749233" target="_blank"><font color="#0000FF">Underwater image enhancement quality evaluation: Benchmark dataset and objective metric</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 32, no. 9, pp. 5959-5974, 2022. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCSVT (2022/04; 2022/09-2023/04; 2023/06-2023/09, 2023/11-2023/12)</i></font></strong><br>
  <a href="https://github.com/yia-yuese/SAUD-Dataset"  target="_blank"><font color="#FF5151">[----SAUD-Dataset & NUIQ-Metric----]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 
	  
<li > Yudong Mao, Qiuping Jiang, <strong>Runmin Cong</strong>, Wei Gao, Feng Shao, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9435962" target="_blank"><font color="#0000FF">Cross-modality fusion and progressive integration network for saliency prediction on stereoscopic 3D images</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 24, pp. 2435-2448, 2022. <br>
</li>
	  
<li > Chun-Le Guo, Qixin Yan, Saeed Anwar, <strong>Runmin Cong</strong>, Wenqi Ren, and Chongyi Li, 
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf" target="_blank"><font color="#0000FF">Image dehazing transformer with transmission-aware 3D position embedding</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 5812-5820, 2022. (CCF A) <br>
<a href="https://li-chongyi.github.io/Proj_DeHamer.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
</li> 	
	  
<li > Guanghui Yue, Wanwan Han, Bin Jiang, Tianwei Zhou, <strong>Runmin Cong</strong>, and Tianfu Wang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9772424" target="_blank"><font color="#0000FF">Boundary constraint network with cross layer feature integration for polyp segmentation</font></a>,
  <ud2>IEEE Journal of Biomedical and Health Informatics</ud2>, vol. 26, no. 8, pp. 4090-4099, 2022. <br>
  <!--<a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li> 

	  
<li > Zhao Wang, Feng Li, <strong>Runmin Cong</strong>, Huihui Bai, and Yao Zhao, 
  <a href="https://link.springer.com/content/pdf/10.1007/s11042-022-12151-4.pdf" target="_blank"><font color="#0000FF">Adaptive feature fusion network based on boosted attention mechanism for single image dehazing</font></a>,
  <ud2>Multimedia Tools and Applications</ud2>, vol. 82, no. 1, pp. 11325-11339, 2022. <br>
</li> 	
	  
<li >  Weiqing Yan, Kaiqi Su,Jinlai Ren, <strong>Runmin Cong</strong>, Shuai Li, and Shuigen Wang,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">Sparse LiDAR and binocular stereo fusion network for 3D object detection</font></a>,
  <ud2>The 5th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</ud2>, 2022. <br>
</li> 	  
	
	 
	  </ul>
  <hr />
	
	
	
<h4><b>2021:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
 
  
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu, Dingwen Zhang, and Qingming Huang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8998588" target="_blank"><font color="#0000FF">ASIF-Net: Attention steered interweave fusion network for RGBD salient object detection</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 51, no. 1, pp. 88-100, 2021. (# co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCyb (2020/12; 2021/01; 2021/03; 2021/04; 2021/06; 2021/12)</i></font></strong><br>
  <a href="https://github.com/Li-Chongyi/ASIF-Net"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"><font color="#FF5151">[Results]</font></a>-->
</li>
	  
<li > Qijian Zhang, <strong>Runmin Cong#</strong>, Chongyi Li, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, and Sam Kwong,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434" target="_blank"><font color="#0000FF">Dense attention fluid network for salient object detection in optical remote sensing images</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 1305-1317, 2021. (CCF A, # co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Hot Paper, ESI Highly Cited Paper, Popular Documents in TIP (2021/01)</i></font></strong> <br>
  <a href="https://rmcong.github.io/proj_DAFNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/EORSSD-dataset"  target="_blank"><font color="#FF5151">[EORSSD Dataset]</font></a>
  <a href="https://github.com/rmcong/DAFNet_TIP20"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
<a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li> 
	
	
<li > Zuyao Chen^, <strong>Runmin Cong^</strong>, Qianqian Xu, and Qingming Huang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9247470" target="_blank"><font color="#0000FF">DPANet: Depth potentiality-aware gated attention network for RGB-D salient object detection</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 7012-7024, 2021. (CCF A, ^ equal contribution)  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_DPANet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
	<a href="https://www.bilibili.com/video/BV1Ry4y1m7WL/"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li> 	  
	  
<li > Chen Zhang, <strong>Runmin Cong*</strong>, Qinwei Lin, Lin Ma, Feng Li, Yao Zhao, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475364" target="_blank"><font color="#0000FF">Cross-modality discrepant interaction network for RGB-D salient object detection</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2094-2102, 2021. (CCF A, * corresponding author, VALSE'2021 Spotlight)<br>
  <a href="https://rmcong.github.io/proj_CDINet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
<a href="https://github.com/1437539743/CDINet-ACM-MM21"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
</li> 	  
	  
<li > Qi Tang, <strong>Runmin Cong*</strong>, Ronghui Sheng, Lingzhi He, Dan Zhang, Yao Zhao, and Sam Kwong,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475373" target="_blank"><font color="#0000FF">BridgeNet: A joint learning network of depth map super-resolution and monocular depth estimation</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2148-2157, 2021. (CCF A, * corresponding author)<br>
  <a href="https://rmcong.github.io/proj_BridgeNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <!--<font color="#999999">[Code]</font>-->
</li> 	   
	  
<li > Dong Jing, Shuo Zhang, <strong>Runmin Cong</strong>, and Youfang Lin,
  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475312" target="_blank"><font color="#0000FF">Occlusion-aware bi-directional guided network for light field salient object detection</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 1692-1701, 2021. (CCF A)
   <a href="https://github.com/Timsty1/OBGNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br
</li> 	 

<li > Hongfa Wen, Chenggang Yan, Xiaofei Zhou, <strong>Runmin Cong</strong>, Yaoqi Sun, Bolun Zheng, Jiyong Zhang, Yongjun Bao, and Guiguang Ding,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9605221" target="_blank"><font color="#0000FF">Dynamic selective network for RGB-D salient object detection</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 9179-9192, 2021. (CCF A) <strong><font color="#00CC33"><i>Popular Documents in TIP (2021/11)</i></font></strong>
  <a href="https://github.com/Brook-Wen/DSNet"  target="_blank"><font color="#FF5151">[Code]</font></a> <br>
  <!--<font color="#999999">[Code]</font>-->
</li> 	 
 
	  
<li > Chongyi Li, Saeed Anwar, Junhui Hou, <strong>Runmin Cong</strong>, Chunle Guo, and Wenqi Ren,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9426457" target="_blank"><font color="#0000FF">Underwater image enhancement via medium transmission-guided multi-color space embedding</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 30, pp. 4985-5000, 2021. (CCF A) <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2021/05-2021/07; 2021/09-2023/01; 2023/03-2024/03)</i></font></strong> <br>
  <a https://li-chongyi.github.io/Proj_Ucolor.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
</li> 	 
	  
	  
<li > Hua Li, Yuheng Jia, <strong>Runmin Cong</strong>, Sam Kwong, and Chuanbo Chen,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292086" target="_blank"><font color="#0000FF">Superpixel segmentation based on spatially constrained subspace clustering</font></a>,
  <ud2>IEEE Transactions on Industrial Informatics</ud2>, vol. 17, no. 11, pp. 7501-7512, 2021. <br>
   <!--<a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
</li> 	  
	
<li > Hua Li, <strong>Runmin Cong*</strong>, Sam Kwong, Chuanbo Chen, Qianqian Xu, and Chongyi Li,
  <a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025521X00023/1-s2.0-S002002552031197X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIAygFBH2m%2B%2Fb52oukNEDOg9vBCQlwVbi1%2BS7Sg9oJSnIAiEA074uSp3%2FDklNmIBwduQ3Ru2%2FABnoiJlUPtqrHK86iNcqtAMIahADGgwwNTkwMDM1NDY4NjUiDE32MwlqAAmZEkI%2F9SqRAxYD9DTwYcm6DxpmygwQTUoy9ndEPxUM%2FXuTChzQP11LAEzmnL26RWw6U6lAHnZVnQb%2B%2Fc%2FVAFHejQzhD8YDCky3sPAzOmqpXVXFHls7QMG2LgGhhuWDQpspfpRhS2MloLePIqVzv15WGH0%2BYIyKwebHPp1ar8tMiNEYQoCHUBz%2B6WQQcQzhEJZoQPLrHQDQNMa59b9tZV9DTm7fU6UJ4TELPT4iHhYujxFZxuKPcmMUvG28I5eau5wIdb%2BCCMngJRHpaTyuzJ70c7p01hafkMZhbznzOL0OWvzfLWHh4na8UxNrnr5SW5oMDE0eSqgSnpwD4yDAWvTfsPehvPuBJyKZWMCd7qyJLRXvr5yOzIXTBGLUzUKaCtJLgS8MUcLTpFHKszUt2dFHjMEEB8pb3BZrqr2XobKwPSekS0krP28wSFA5pAHrnfMWliyj9kZBnyVdurnjmcOrSknUx1dDonY6GCPpCVSHrGH6qNN8gUwmPpRnJxPNNRjo4GCFuXLHdY8qxQgiQmbUzJej%2F0nBHXSDMN3imIAGOusBq1tMJqeUTsrIVpUUtv9GU5gpG8%2FK%2F5gHthUou7%2Bfx8aTyuRWUcZ9GF1nhHPFI49MXUiGRbW9qS3UWnTfrzYNl9e5hoTof5jr7CK74BlzDBRHqHRVYweYNr6PpkZopDSvorW40zAzwpBxXIMA1XSLIgntBZNTqBMwiWea9ulQek6fVLkV0RmU357N7Ets15p7ivDDoU2ZfyE%2BiXEp5c2%2FBFME%2BYeYavuOxBEeSZH5AaCyUqCjM8oX98%2Br4%2BTpWz8GzPMOYmKwg69u7gYZoPS4GXXMzD0FNzhXgc8QECZQm%2BD5fRSEH75WSKOOQA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210119T015339Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZVTASF26%2F20210119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0bb9b9266999431dfd680d53ccfc58cfcd456f8bf9f75c122380c6260de1cebe&hash=829bfa37bd56fea063d166b49a7466f05c59b7570955e5014ab9d02c8c60338a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S002002552031197X&tid=spdf-03751d9c-ee4f-4c51-8986-c6a1f06bcf9a&sid=2743a5db222cd54e78095df16e98e6012296gxrqa&type=client" target="_blank">
	  <font color="#0000FF">Stereo superpixel: An iterative framework based on parallax consistency and collaborative optimization</font></a>,
  <ud2>Information Sciences</ud2>, vol. 556, pp. 209-222, 2021. (* corresponding author) <br>
   <!--<a href="https://github.com/JosephChenHub/DPANet"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
</li> 	
	  
	  
<li > Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, <strong>Runmin Cong</strong>, Chunjie Zhang, Chunyu Lin, Meiqin Liu, and Yao Zhao,
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_Towards_Fast_and_Accurate_Real-World_Depth_Super-Resolution_Benchmark_Dataset_and_CVPR_2021_paper.pdf" target="_blank"><font color="#0000FF">Towards fast and accurate real-world depth super-resolution: Benchmark dataset and baseline</font></a>,
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 9229-9238, 2021. (CCF A, VALSE'2021 Spotlight) <br>
   <a href="http://mepro.bjtu.edu.cn/resource.html"  target="_blank"><font color="#FF5151">[RGB-D-D Dataset]</font></a>
   <a href="https://github.com/lingzhi96/RGB-D-D-Dataset"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li> 	

<li > Ning Yang, Qihang Zhong, Kun Li, <strong>Runmin Cong*</strong>, Yao Zhao, and Sam Kwong,
  <a href="https://pdf.sciencedirectassets.com/271519/1-s2.0-S0923596521X00033/1-s2.0-S0923596521000503/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBkaCXVzLWVhc3QtMSJIMEYCIQCZufipe4utlPRuvKkkM2335wR%2BcJ466YEGMmffQojgkgIhAJldmoqsTzrj28ZhX1rg3yCryOYuBNyChlnktI6RTzh%2FKrQDCFIQAxoMMDU5MDAzNTQ2ODY1IgxvUIVhIu2NaVYCIHMqkQNfU0g4vtc1K7bzI2uNv5OuV8BrIAvbPuxOUKMAPFSdmO9sMi6rKZiaVOslZpIra%2Fp54REUwAMFobL9F1g3xwyBNjNTmRRVYnWJuX6zOTMjtri9mG4qhzcH6307co3vuPJenqjtdDWBZt%2FPx3UUBvCNuKOFER4KyI5fNp%2Bv5QF19jA86%2BL2tO6XtlD5Ux3M3KRtpeZlGGfm5t9TNlOyEGllkQg9gJVBL8uCsAQmB84H0uCyYaHriGqDKz%2BgmXySlpfwLDkAF526m2A48%2BCreoNu4%2FeQ0N8nP7ifSuEV3w8ev9vWznJd80OH7o6FyStjbMjr8ZNPbnYDk5deupsVqN%2BTAHkDhCG%2B4EAZGTGzl26lU36RqjDvMz0ELWxunHWr%2B9I4pQUzGESoxakERYs%2BAoZg0fbduwF6Edn2FIQCR1aFV2T4tbTPkI6sre3ZjjXz7XGl69QojAKMA9%2FwIl%2FO%2BOZEHQZtnD13YWDlv07LWPQy%2BPmVgQVWzXIAfFUn5cKeCTRB5o70iu4QPFkq4Eg6zxXyLjDM7c%2BCBjrqAWJ8kSgwF7dnybnKYbakF9LI7W9IzjM0fjqHBzb1uQGyu8NUa8no850HN204MuhS5q7iK6xInN0U%2FlVaoE%2FfISOm%2FcFWaAgAYb8WEgQZ5TXm2sUnW%2FbfXTbhIJ54l9bcGCPom8Y5oWKCfPnvdxGFRPGMKfMrE5eIuXH2WEajpsLulUYyoMEAsyIeXQgNBQWxHZ1wCF9fZGV7eYKaogxI%2BHNTogOr%2F6SOg6el8%2FcHHqWyoPE%2BWlCbHs%2BNBYeP7IHJJJNAWUf3Wqvaxj4Ek%2Fox1OgDBIQxRRh%2F4%2FmWPJtxv8LLzkdjvVJIPNGrDw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210319T015224Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYQRBHFMGN%2F20210319%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=c3614abab9ddf93ae556038ab84299f41d1b96d17620256b45dd2c60092f4dcc&hash=0d54c4e2714038ea8368b3c3c9d650b5e101fda1da545c2f1abe6273ea0324e7&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0923596521000503&tid=spdf-0a645599-32f1-472c-aef7-40038f476093&sid=ec8c75165ab1a645ad398fb410c52314e2a1gxrqa&type=client">
	  <font color="#0000FF">A reference-free underwater image quality assessment metric in frequency domain</font></a>,
  <ud2>Signal Processing: Image Communication</ud2>, vol. 94, pp. 1-10, 2021. (* corresponding author) <br>
   <a href="https://pan.baidu.com/s/12GHKKUCRRoR1LzhsKcSNRQ"  target="_blank"><font color="#FF5151">[UWIQA Dataset] (code: MVPL)</font></a>
	 <a href="https://github.com/rmcong/Code-for-FDUM-method"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li>
													
<li > Xuan Liu, Yumo Zhang, <strong>Runmin Cong*</strong>, Chen Zhang, Ning Yang, Chunjie Zhang, and Yao Zhao,
  <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-88007-1_48.pdf" target="_blank"><font color="#0000FF">GGRNet: Global graph reasoning network for salient object detection in optical remote sensing images</font></a>,
  <ud2>The 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</ud2>, pp. 584–596, 2021. (* corresponding author)  <br>
</li> 	  
											   

<li > Junkang Hu, Qiuping Jiang, <strong>Runmin Cong</strong>, Wei Gao, and Feng Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9496260" target="_blank"><font color="#0000FF">Two-branch deep neural network for underwater image enhancement in HSV color space</font></a>,
  <ud2>IEEE Signal Processing Letters</ud2>, vol. 28, pp. 2152-2156, 2021. <strong><font color="#00CC33"><i>Popular Documents in SPL (2022/09; 2022/10)</i></font></strong><br>
</li> 	  	  
	 
	  </ul>
  <hr />
	
	
<h4><b>2020:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">  

<li > <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Junhui Hou, Qingming Huang, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8807367" target="_blank"><font color="#0000FF">Going from RGB to RGBD saliency: A depth-guided transformation model</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 50, no. 8, pp. 3627-3639, 2020. 
  <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br>
  <a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-DTM-Method"><font color="#FF5151">[Code]</font></a></li>
		
<li > Qijian Zhang, <strong>Runmin Cong#</strong>, Junhui Hou, Chongyi Li, and Yao Zhao, 
  <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf" target="_blank"><font color="#0000FF">CoADNet: Collaborative aggregation-and-distribution networks for co-salient object detection</font></a>,
  <ud2>Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</ud2>, pp. 6959-6970, 2020. (CCF A, # co-first and corresponding author) <br>
  <a href="https://rmcong.github.io/proj_CoADNet.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>-->
  <!--<a href="https://github.com/Li-Chongyi/ASIF-Net"><font color="#FF5151">[Results]</font></a>-->
  <a href="https://github.com/rmcong/CoADNet_NeurIPS20"  target="_blank"><font color="#FF5151">[Code & Results]</font></a>
<a href="https://www.bilibili.com/video/BV1np4y1z7B7"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a>
</li>
	
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Yongri Piao, Qianqian Xu, and Chen Change Loy, 
  <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf" target="_blank"><font color="#0000FF">RGB-D salient object detection with cross-modality modulation and selection</font></a>,
  <ud2>European Conference on Computer Vision (ECCV)</ud2>, pp. 225-241, 2020. (# co-first and corresponding author) <br>
  <a href="https://li-chongyi.github.io/Proj_ECCV20"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <a href="https://github.com/Li-Chongyi/cmMS-ECCV20"  target="_blank"><font color="#FF5151">[Code]</font></a>
  </li>
	
<li > Chongyi Li, <strong>Runmin Cong#</strong>, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, 
  <a href="https://www.sciencedirect.com/science/article/pii/S0925231220313692?dgcid=author" target="_blank"><font color="#0000FF">A parallel down-up fusion network for salient object detection in optical remote sensing images</font></a>,
  <ud2>Neurocomputing</ud2>, vol. 415, pp. 411-420, 2020. (# co-first and corresponding author) <br>
  <!--<a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>
  <font color="#999999">[Results]</font>--> 
</li>
	

<li > Chongyi Li, Huazhu Fu, <strong>Runmin Cong*</strong>, Zechao Li, and Qianqian Xu, 
  <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413928" target="_blank"><font color="#0000FF">NuI-Go: Recursive non-local encoder-decoder network for retinal image non-uniform illumination removal</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 1478-1487, 2020. (CCF A, * corresponding author) <br>
  <a href="https://li-chongyi.github.io/Proj_ACMMM20_NuI-Go.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  </li>

<li > Peisong Wen, Ruolin Yang, Qianqian Xu, Chen Qian, Qingming Huang, <strong>Runmin Cong</strong>, and Jianlou Si, 
  <a href="https://dl.acm.org/doi/10.1145/3394171.3414035" target="_blank"><font color="#0000FF">DMVOS: Discriminative matching for real-time video object segmentation</font></a>,
  <ud2>ACM International Conference on Multimedia (ACM MM)</ud2>, pp. 2048-2056, 2020. (CCF A)<br>
  <!--<a href="https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a> 
  <font color="#999999">[Code]</font>-->
</li>
	
	
<li > Yawen Huang, Feng Zheng, <strong>Runmin Cong</strong>, Weilin Huang, Matthew R. Scott, and Ling Shao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9152126" target="_blank"><font color="#0000FF">MCMT-GAN: Multi-task coherent modality transferable GAN for 3D brain image synthesis</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 29, pp. 8187-8198, 2020. <br> 
</li>

<li > Feng Li^, <strong>Runmin Cong^</strong>, Huihui Bai, and Yifan He, 
  <a href="https://www.ijcai.org/Proceedings/2020/0075.pdf" target="_blank"><font color="#0000FF">Deep interleaved network for image super-resolution with asymmetric co-attention</font></a>,
  <ud2>International Joint Conference on Artificial Intelligence (IJCAI)</ud2>, pp. 534-543, 2020. (CCF A, ^ equal contribution) <br>
  <a href="https://github.com/lifengshiwo/DIN"  target="_blank"><font color="#FF5151">[Code]</font></a>
</li>
	
<li> Ping Han, Binbin Han, Xiaoguang Lu, <strong>Runmin Cong*</strong>, and Dandan Sun, 
  <a href="https://www.tandfonline.com/doi/full/10.1080/01431161.2019.1643939" target="_blank"><font color="#0000FF">Unsupervised classification of PolSAR images based on multi-level feature extraction</font></a>, 
  <ud2>International Journal of Remote Sensing</ud2>, vol. 41, no. 2, pp. 534-548, 2020. (* corresponding author)  <br></li>	  

<li > Chongyi Li, Chunle Guo, Wenqi Ren, <strong>Runmin Cong</strong>, Junhui Hou, Sam Kwong, and Dacheng Tao,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8917818" target="_blank"><font color="#0000FF">An underwater image enhancement benchmark dataset and beyond</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 29, pp. 4376-4389, 2020.  <strong><font color="#00CC33"><i>ESI Hot Paper, ESI Highly Cited Paper, Popular Documents in TIP (2020/04-2024/03)</i></font></strong><br>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[UIEBD Dataset]</font></a>
  <a href="https://li-chongyi.github.io/proj_benchmark.html"  target="_blank"><font color="#FF5151">[Code and Results]</font></a><br></li>

<li> Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and <strong>Runmin Cong</strong>, 
  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank"><font color="#0000FF">Zero-reference deep curve estimation for low-light image enhancement</font></a>, 
  <ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2>, pp. 1780-1789, 2020. (CCF A) 
  <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/Li-Chongyi/Zero-DCE"><font color="#FF5151">[Code]</font></a></li>
			
<li> Zuyao Chen, Qianqian Xu, <strong>Runmin Cong</strong>, and Qingming Huang, 
  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6633" target="_blank"><font color="#0000FF">Global context-aware progressive aggregation network for salient object detection</font></a>, 
  <ud2>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</ud2>, pp. 10599-10606, 2020. (CCF A, <font color="#FF5151">Oral Presentation</font>) 
  <a href="https://github.com/JosephChenHub/GCPANet"><font color="#FF5151">[Code]</font></a></li>
			
<li> Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8792133" target="_blank"><font color="#0000FF">PDR-Net: Perception-inspired single image dehazing network with refinement</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 22, no. 3, pp. 704-716, 2020. <strong><font color="#00CC33"><i>Popular Documents in TMM (2020/03-2022/05; 2020/07-2020/09; 2020/12)</i></font></strong><br></li>
			
<li> Mengxin Han, <strong>Runmin Cong</strong>, Xinyu Li, Huazhu Fu, and Jianjun Lei, 
  <a href="https://reader.elsevier.com/reader/sd/pii/S0167865518307748?token=72E89BF2F7EAA411988F3F266DD1A9872321E794C0F05651EF5297ACB414955DDF0FA962EB3DD0403CBDC0CC9A11EDFC" target="_blank"><font color="#0000FF">Joint spatial-spectral hyperspectral image classification based on convolutional neural network</font></a>,
  <ud2>Pattern Recognition Letters</ud2>, vol. 130, pp. 38-45, 2020. <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong><br></li>
			
<li> Ling Du, Anthony T.S. Ho, and <strong>Runmin Cong</strong>, 
  <a href="https://pdf.sciencedirectassets.com/271519/1-s2.0-S0923596519X00107/1-s2.0-S0923596519301286/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJIMEYCIQCN2vQCvUAL01W5V7K%2FnMV3sjrPNJMAFD0fLhlvaMT3kQIhAKn4XPKexGavuJYT0NXPDLrJl9MZkirT9beXgRDVi9OXKrQDCBMQAhoMMDU5MDAzNTQ2ODY1IgxPak1%2BdhrHClpchp8qkQM4BGURXfAFxKW7eVxcX6HuivgNZG7xUz1VIeLDy1CripP2DQhNb94zRLxHF8zmYnfjlw2dgCsGLWYKwWdTaeFVt6CMr4osKu%2BGS5jPV30PM9WJ3wb39eCf9GdlkjF6ChfNNaKok8K9deAD4czJDfl5JWD027lAikfHMgiK0DIEOVIrn6o6p%2B1EeLZbBdjhKjJDXJrNNaP7tLgxN%2FJtgIz%2FV6w2SYnzIBM36iF1%2Fw6Zn1Wzeqq1LOrcTAB0yid2Tspg84bibCs4APBsCU99WEvaKmBwW2tDlyArqGdm9ZC4zSCySvlnF3KyDCpUyeukAQy%2Bt1mMOJrKIXSyhLBOJ%2BiOe9El0p54zOUtfXn4Z%2BLwtG38HHzFepTsv7zueM4xZgG87cgE0F4cO1qFhvrLM6HPUjZknG2%2FN3eTbVgGcGcZ9TRXBZNFkibeEwqTlD89gtfngrQIRr4%2BYDnSHGor7OOMnO6V9fI1ADkxiTQS%2Bq5i1rK3KAcGc6OlWd5nL7%2BQefU%2BeUBut4PuniIX62qCurXOlzCHl%2B7yBTrqAbZNeL4Ts5ZDExDd1uvAlwIbiKlsrMYGsbFdbG0jJarammkD5%2Fsv7fe7GHj8VB5Cpo9cNTC8682p2Rf1Wkv%2FJy7kFLTAM9G5iNG8SmaA2%2BML2OW0%2FRDogjMAH%2Bj1V0RktBRc6QhwiB4bZlIjtSMb18u69h1wNaTrQLEpK4cjWrKhE1GUpbHJdeZvqCd4xjcr99wqdKz5lw7q24Kzkx4C2cosq1snLcQAvP12gudaeI6qHBgQUqOzKDaSFYzr4EC%2Bvf12t7B872BYkE93z0KMd6Gd0PoD72PLu1woGXgW%2BWGQ7abbBnyWXPebCQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200301T105922Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRI43AGMU%2F20200301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b2fb4a7682437c2bde4361f6284b553747c19ac05327209f2733727799fe8376&hash=9eaa2b4f295be43fde971320b8ea29401b403055a2f1b600861747f129cb1faa&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0923596519301286&tid=spdf-a1effdbd-edfc-4cf9-bb67-a07c54b9ba77&sid=42f01d2749d9e849294bb514e02324dda678gxrqa&type=client" target="_blank"><font color="#0000FF">Perceptual hashing for image authentication: A survey</font></a>, 
  <ud2>Signal Processing: Image Communication</ud2>, vol. 81, pp. 1-23, 2020. <br></li>	  
		
<li> <strong>丛润民</strong>, 张禹墨, 张晨, 李重仪, 赵耀,
  <a href="https://rmcong.github.io/Publication.html" target="_blank"><font color="#0000FF">深度学习驱动的水下图像增强与复原研究进展</font></a>, 
  <ud2>信号处理</ud2>, 36(9): 1377-1389, 2020. 
	<strong><font color="#00CC33"><i>2020-2022年度优秀论文（2020年仅2篇，2/231=0.87%）、2023年中国电子学会优博论坛优秀墙报奖</i></font></strong><br>
</li>	  

  
	</ul>
			<hr />
				
<h4><b>2019:</b></h4>
<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">
<li > <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Weisi Lin, Qingming Huang, Xiaochun Cao, and Chunping Hou, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8116754" target="_blank"><font color="#0000FF">An iterative co-saliency framework for RGBD images</font></a>,
  <ud2>IEEE Transactions on Cybernetics</ud2>, vol. 49, no. 1, pp. 233-246, 2019. <br/>
  <a href="https://rmcong.github.io/proj_RGBD_cosal_tcyb.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/2017-TCyb-An-iterative-RGBD-co-saliency-framework"  target="_blank"><font color="#FF5151">[Results]</font></a><br></li>

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Fatih Porikli, Qingming Huang, and Chunping Hou,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8704996" target="_blank"><font color="#0000FF">Video saliency detection via sparsity-based reconstruction and propagation</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 28, no. 10, pp. 4819-4831, 2019. <strong><font color="#00CC33"><i>Popular Documents in TIP (2019/08)</i></font></strong><br></li>
  <a href="https://rmcong.github.io/proj_video_sal_SRP_tip.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-SRP-Method"><font color="#FF5151">[Code and Results]</font></a>
  <a href="demo/video_sal_SRP_demo.avi"  target="_blank"><font color="#FF5151">[Demo]</font></a>  
	  
<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, and Nam Ling, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8556071" target="_blank"><font color="#0000FF">HSCS: Hierarchical sparsity based co-saliency detection for RGBD images</font></a>, 
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 21, no. 7, pp. 1660-1671, 2019. <br/>
  <a href="https://rmcong.github.io/proj_RGBD_cosal_HSCS_tmm.html" target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-HSCS-Method"><font color="#FF5151">[Code]</font></a>
  <a href="https://github.com/rmcong/Results-for-2018TMM-HSCS" target="_blank"><font color="#FF5151">[Results]</font></a><br></li>

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng, Weisi Lin, and Qingming Huang, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466906" target="_blank"><font color="#0000FF">Review of visual saliency detection with comprehensive information</font></a>,
  <ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2>, vol. 29, no. 10, pp. 2941-2959, 2019. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TCSVT (2019/10-2020/01; 2020/03; 2020/06; 2021/01; 2021/03; 2021/05; 2021/06; 2021/11; 2022/03)</i></font></strong><br></li>  
    
<li> Chongyi Li, <strong>Runmin Cong#</strong>, Junhui Hou, Sanyi Zhang, Yue Qian, and Sam Kwong, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793227" target="_blank"><font color="#0000FF">Nested network with two-stream pyramid for salient object detection in optical remote sensing images</font></a>, 
  <ud2>IEEE Transactions on Geoscience and Remote Sensing</ud2>, vol. 57, no. 11, pp. 9156-9166, 2019. (# co-first and corresponding author) <strong><font color="#00CC33"><i>ESI Highly Cited Paper</i></font></strong> <br>
  <a href="https://li-chongyi.github.io/proj_optical_saliency.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/ORSSD-dataset"  target="_blank"><font color="#FF5151">[ORSSD Dataset]</font></a>
<a href="https://www.bilibili.com/video/BV1ev4y1D7aa/?vd_source=6432d91082e1f71a4035ed4c6a875caa"  target="_blank"><font color="#FF5151">[中文讲解视频]</font></a></li>
  	  
<li> Chunle Guo, Chongyi Li, Jichang Guo, <strong>Runmin Cong</strong>, Huazhu Fu, and Ping Han, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8579111" target="_blank"><font color="#0000FF">Hierarchical features driven residual learning for depth map super-resolution</font></a>,
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 28, no. 5, pp. 2545-2557, 2019. 
  <a href="https://li-chongyi.github.io/proj_SR.html" target="_blank"><font color="#FF5151">[----Project Page----]</font></a></li>

<li> Hua Li, Sam Kwong, Chuanbo Chen, Yuheng Jia, and <strong>Runmin Cong</strong>, 
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8673630" target="_blank"><font color="#0000FF">Superpixel segmentation based on square-wise asymmetric segmentation and structural approximation</font></a>,
  <ud2>IEEE Transactions on Multimedia</ud2>, vol. 21, no. 10, pp. 2625-2637, 2019. <br></li>
 	  
	  
	  
</ul>
	<hr />
  
<h4><b>2018:</b></h4>
		<ul class="graid3-ul">
  <div style="text-align: justify; display: block; margin-right: auto;">

<li> <strong>Runmin Cong</strong>, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, and Chunping Hou,
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8070326" target="_blank" ><font color="#0000FF">Co-saliency detection for RGBD images based on multi-constraint feature matching and cross label propagation</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 27, no. 2, pp. 568-579, 2018. <br>
  <a href="https://rmcong.github.io/proj_RGBD_cosal.html" target="_blank" ><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/RGBD-Cosal150-Dataset"  target="_blank" ><font color="#FF5151">[Dataset]</font></a>
  <a href="https://github.com/rmcong/Results-for-2018TIP-RGBD-Co-saliency"  target="_blank" ><font color="#FF5151">[Results]</font></a><br></li>   

<li> <strong>丛润民</strong>, 雷建军, 付华柱, 王文冠, 黄庆明, 牛力杰,
  <a href="http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5560&journal_id=jos" target="_blank"><font color="#0000FF">视频显著性检测研究进展</font></a>,
  <ud2>软件学报</ud2>, 29(8): 2527−2544, 2018. (CCF A, EI, <font color="#FF5151"><strong>第十五届北京青年优秀科技论文奖</strong></font>)<br></li>

<li> Yonghua Zhang, Liang Li, <strong>Runmin Cong</strong>, Xiaojie Guo, Hui Xu, and Jiawan Zhang,
  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8486603" target="_blank" ><font color="#0000FF">Co-saliency detection via hierarchical consistency measure</font></a>, 
  <ud2>IEEE International Conference on Multimedia & Expo (ICME)</ud2>, pp. 1-6, 2018. <font color="#FF5151">Oral Presentation, <strong>Best Student Paper Runner-Up (rate: 2/582)</strong></font><br></li> 
  </ul>
  <hr />
			
<h4><b>2017:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">  

<li> Min Ni, Jianjun Lei, <strong>Runmin Cong*</strong>, Kaifu Zheng, Bo Peng, and Xiaoting Fan, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8106766" target="_blank"><font color="#0000FF">Color-guided depth map super resolution using convolutional neural network</font></a>, 
  <ud2>IEEE Access</ud2>, vol. 2, pp. 26666-26672, 2017. (* corresponding author) <br></li>
  
<li> Chongyi Li, Jichang Guo, Chunle Guo, <strong>Runmin Cong</strong>, and Jiachang Gong, 
  <a href="https://reader.elsevier.com/reader/sd/pii/S016786551730171X?token=6A97A6E5D8235CD901B3FCDDA0C23B76726B7DA66091936F46C1DF69DB05A08CB47410E83AC3BC041DAFF11B0254B376" target="_blank"><font color="#0000FF">A hybrid method for underwater image correction</font></a>,
  <ud2>Pattern Recognition Letters</ud2>, vol. 94, pp. 62-67, 2017.  <br></li>
  </ul>
			<hr />
			
<h4><b>2016:</b></h4>
		<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	
<li> <strong>Runmin Cong</strong>, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, and Chunping Hou, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7457641" target="_blank"><font color="#0000FF">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</font></a>, 
  <ud2>IEEE Signal Processing Letters</ud2>, vol. 23, no. 6, pp. 819-823, 2016. <strong><font color="#00CC33"><i>Popular Documents in SPL (2016/05-2016/10)</i></font></strong> <br>
  <a href="https://rmcong.github.io/proj_RGBD_sal.html"  target="_blank"><font color="#FF5151">[----Project Page----]</font></a>
  <a href="https://github.com/rmcong/Code-for-DCMC-method"  target="_blank"><font color="#FF5151">[Code]</font></a><br></li>
  
<li> <strong>Runmin Cong</strong>, Ping Han, Chongyi Li, Jiaji He, and Zaiji Zhang, 
  <a href="https://www.researchgate.net/publication/308665367_Manmade_target_extraction_based_on_multistage_decision_and_its_application_for_change_detection_in_polarimetric_synthetic_aperture_radar_image?_iepl%5BviewId%5D=NimQuYvO7KOlf738gXMw8OZYLNFxI1Mop5xZ&_iepl%5Bcontexts%5D%5B0%5D=prfhpi&_iepl%5Bdata%5D%5BstandardItemCount%5D=3&_iepl%5Bdata%5D%5BuserSelectedItemCount%5D=0&_iepl%5Bdata%5D%5BtopHighlightCount%5D=2&_iepl%5Bdata%5D%5BstandardItemIndex%5D=1&_iepl%5Bdata%5D%5BstandardItem1of3%5D=1&_iepl%5BtargetEntityId%5D=PB%3A308665367&_iepl%5BinteractionType%5D=publicationTitle" target="_blank"><font color="#0000FF">Manmade target extraction based on multi-stage decision and its application for change detection in polarimetric synthetic aperture radar image</font></a>,
  <ud2>Journal of Electronic Imaging</ud2>, vol. 25, no. 5, pp. 1-13, 2016. <br></li> 
  
<li> Chongyi Li, Jichang Guo, <strong>Runmin Cong</strong>, Yanwei Pang, and Bo Wang, 
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7574330" target="_blank"><font color="#0000FF">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</font></a>, 
  <ud2>IEEE Transactions on Image Processing</ud2>, vol. 25, no. 12, pp. 5664-5677, 2016. <strong><font color="#00CC33"><i>ESI Highly Cited Paper, Popular Documents in TIP (2022/10)</i></font></strong>
  <a href="https://github.com/Li-Chongyi/TIP2016-code"  target="_blank"><font color="#FF5151">[Code]</font></a></li>
  
<li> Chongyi Li, Jichang Guo, Bo Wang, <strong>Runmin Cong</strong>, Yan Zhang, and Jian Wang, 
  <a href="https://www.researchgate.net/publication/303888472_Single_underwater_image_enhancement_based_on_color_cast_removal_and_visibility_restoration" target="_blank"><font color="#0000FF">Single underwater image enhancement based on color cast removal and visibility restoration</font></a>, 
  <ud2>Journal of Electronic Imaging</ud2>, vol. 25, no. 3, pp. 1-16, 2016. <br>
  </li>
</ul>
			<hr />
<!--
<h4><b>2015:</b></h4>
		<ul class="graid3-ul">
 <div style="text-align: justify; display: block; margin-right: auto;">
	 
<li> Ping Han, <strong>Runmin Cong</strong>, and Zaiji Zhang, 
  <font color="#0000FF">Change detection algorithm of polarimetric SAR image based on polarization state extracting</font>,
  <ud2>Systems Engineering and Electronics</ud2>, vol. 37, no. 7, pp. 1526-1530, 2015. (in Chinese, EI)<br></li>
  </ul>
				<hr />
-->
			
<h4><b>授权中国发明专利:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	 
<li>一种立体视觉显著性检测方法，专利号：ZL 201610244589.9，申请日：2016.04.19，授权公告日：2018.08.31</li>
<li>一种RGB-D图像显著性目标检测方法，专利号：ZL 202110872457.1，申请日：2021.07.30，授权公告日：2023.07.28</li>
<li>一种RGB-D图像显著性目标检测方法，专利号：ZL 202010199264.X，申请日：2020.03.20，授权公告日：2023.08.30</li>
<li>一种图间显著性检测方法，专利号：ZL 201710942099.0，申请日：2017.10.11，授权公告日：2021.04.16</li>
<li>一种深度图可靠性评价测度方法，专利号：ZL 201610242241.6，申请日：2016.04.19，授权公告日：2018.08.10</li>	 
<li>一种迭代协同显著性检测方法，专利号：ZL 201711064083.0，申请日：2017.11.02，授权公告日：2021.06.04</li>
<li>一种协同显著性检测方法，专利号：ZL 201710942783.9，申请日：2017.10.11，授权公告日：2021.06.04</li>
<li>一种深度形状先验提取方法，专利号：ZL 201711065005.2，申请日：2017.11.02，授权公告日：2021.04.30</li>
<li>一种 RGBD 图像协同显著性检测方法，专利号：ZL 201810879724.6，申请日：2018.08.03，授权公告日：2021.09.17</li>
<li>一种RGB显著性到RGBD显著性的转换方法，专利号：ZL 201910375809.5，申请日：2019.05.07，授权公告日：2023.04.18</li>	
<li>一种视频显著性检测方法，专利号：ZL 201910266112.4，申请日：2019.04.03，授权公告日：2023.02.07</li>	
<li>一种立体图像重定向方法，专利号：ZL 201610874827.4，申请日：2016.09.30，授权公告日：2019.12.06</li>
<li>一种深度视频快速帧内编码方法，专利号：ZL 201810317701.6，申请日：2018.04.10，授权公告日：2021.04.30</li>
<li>一种联合场景和运动多特征的视频行为聚类方法，专利号：ZL 201810962264.3，申请日：2018.08.22，授权公告日：2021.06.04</li>
<li>深度图超分辨率重建方法，专利号：ZL 201610727602.6，申请日：2016.08.25，授权公告日：2019.10.18</li>	
<li>一种2D转3D深度估计方法，专利号：ZL 201610780883.1，申请日：2016.08.31，授权公告日：2019.06.04</li>	 	 
<li>一种立体图像匹配图计算方法，专利号：ZL 201610780786.2，申请日：2016.08.31，授权公告日：2019.05.31</li>	
<li>一种基于最优化颜色修正和回归模型的水下图像复原方法，专利号：ZL 201610606187.9，申请日：2016.07.25，授权公告日：2019.03.29</li>  
<li>一种屏幕内容与自然内容划分及快速编码方法，专利号：ZL 201611031480.3，申请日：2016.11.18，授权公告日：2019.01.29</li>
<li>一种基于虚拟视点绘制质量的深度图上采样方法，专利号：ZL 201610751851.9，申请日：2016.08.27，授权公告日：2019.08.02</li>	
<li>一种基于协同注意力的草图图像检索方法，专利号：ZL 201910746351.X，申请日：2019.08.13，授权公告日：2022.11.15</li>	
<li>一种基于半异构联合嵌入网络的草图图像检索方法，专利号：ZL 201910746354.3 申请日：2019.08.13，授权公告日：2022.12.02</li>	
	 
	<!-- 
<li><strong>丛润民</strong>，雷建军，侯春萍，李重仪，贺小旭，段金辉. 一种立体视觉显著性检测方法，专利号：ZL 201610244589.9，申请日：2016.04.20，授权公告日：2018.08.31
</li>

<li>雷建军，<strong>丛润民</strong>，侯春萍，段金辉，李东阳. 一种深度图可靠性评价测度，专利号：ZL 201610242241.6，申请日：2016.04.20，授权公告日：2018.08.10
 </li>	 

<li>雷建军，<strong>丛润民</strong>，侯春萍，张三义，陈越，郭琰. 一种迭代协同显著性检测方法，专利号：ZL 201711064083.0，申请日：2017.11.02，授权公告日：2021.06.04</li>
<li>雷建军，<strong>丛润民</strong>，侯春萍，张静，范晓婷，彭勃. 一种协同显著性检测方法，专利号：ZL 201710942783.9，申请日：2017.10.11，授权公告日：2021.06.04</li>
<li>雷建军，<strong>丛润民</strong>，侯春萍，李欣欣，韩梦芯，罗晓维. 一种深度形状先验提取方法，专利号：ZL 201711065005.2，申请日：2017.11.02，授权公告日：2021.04.30</li>
<li>雷建军，张凯明，孙振燕，彭勃，<strong>丛润民</strong>，张曼华，徐遥令. 一种深度视频快速帧内编码方法，专利号：ZL 201810317701.6，申请日：2018.04.10，授权公告日：2021.04.30</li>
<li>雷建军，彭勃，郑泽勋，贾亚龙，<strong>丛润民</strong>，张静. 一种联合场景和运动多特征的视频行为聚类方法，申请号：201810962264.3，申请日：2018.08.22，授权公告日：2021.06.04</li>


<li>郭继昌，李重仪，<strong>丛润民</strong>，郭春乐，顾翔元.一种基于最优化颜色修正和回归模型的水下图像复原方法，专利号：ZL 201610606187.9，申请日：2016.07.25，授权公告日：2019.03.29
 </li> 

<li>雷建军，吴敏，侯春萍，<strong>丛润民</strong>，李乐乐，郭琰. 一种立体图像重定向方法，专利号：ZL 201610874827.4，申请日：2016.09.30，授权公告日：2019.12.06
 </li>
	 
<li>雷建军，李乐乐，侯春萍，<strong>丛润民</strong>，张凝，吴敏. 一种基于虚拟视点绘制质量的深度图上采样方法，专利号：ZL 201610751851.9，申请日：2016.08.27，授权公告日：2019.08.02
 </li>	 
	 
<li>雷建军，李乐乐，侯春萍，吴敏，<strong>丛润民</strong>，倪敏. 深度图超分辨率重建方法，专利号：ZL 201610727602.6，申请日：2016.08.25，授权公告日：2019.10.18
</li>	
	 
<li>吴敏，雷建军，侯春萍，李乐乐，<strong>丛润民</strong>，梅旭光. 一种立体图像匹配图计算方法，专利号：ZL 201610780786.2，申请日：2016.08.31，授权公告日：2019.05.31
</li>	
	 
<li>雷建军，李东阳，侯春萍，孙振燕，<strong>丛润民</strong>，彭勃. 一种屏幕内容与自然内容划分及快速编码方法，专利号：ZL 201611031480.3，申请日：2016.11.18，授权公告日：2019.01.29
   </li>
	 
<li>雷建军，张凝，侯春萍，张翠翠，郑凯夫，<strong>丛润民</strong>. 一种2D转3D深度估计方法，专利号：ZL 201610780883.1，申请日：2016.08.31，授权公告日：2019.06.04
 </li>	 	 
	--> 
  </ul>	
	<hr />
			
<ul class="graid3-ul">
        <div style="text-align: justify; display: block; margin-right: auto;">
	Here are the Impact Factors (IF) of selected journals of my publications. <br>
        - IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI): 20.8 <br>
	- International Journal of Computer Vision (IJCV): 11.6 <br>
	- IEEE Transactions on Industrial Informatics (TII): 11.7<br>
	- IEEE Transactions on Image Processing (TIP): 10.8 <br>
	- IEEE Transactions on Neural Networks and Learning Systems (TNNLS): 10.2<br>
	- IEEE Transactions on Cybernetics (TCyb): 9.4 <br>
	- IEEE Transactions on Intelligent Transportation Systems (TITS): 7.9  <br>
	- IEEE Transactions on Multimedia (TMM): 8.4 <br> 
	- IEEE Transactions on Geoscience and Remote Sensing (TGRS): 7.5 <br>
	- IEEE Transactions on Circuits and Systems for Video Technology (TCSVT): 8.3 <br>
	- IEEE Transactions on Automation Science and Engineering (TASE): 5.9 <br>
	- IEEE Transactions on Instrumentation and Measurement (TIM): 5.6 <br>
	- IEEE Transactions on Computational Imaging (TCI): 4.2 <br>
	- IEEE Transactions on Consumer Electronics (TCE): 4.3 <br>
	- IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI): 4.851 <br>
	- IEEE Journal of Biomedical and Health Informatics (JBHI): 7.021<br>
	- SCIENCE CHINA Information Sciences (SCIS): 7.275<br>
	- ACM Transactions on Multimedia Computing Communications and Applications (TOMM): 4.094<br>
	- Information Sciences: 8.233 <br>
	- Neurocomputing: 5.779 <br>
	- Pattern Recognition Letters (PRL): 4.757 <br>
	- IEEE Signal Processing Letters (SPL): 3.201 <br>
	- International Journal of Remote Sensing (IJRS): 3.531	 <br>
	- Signal Processing: Image Communication (SPIC): 3.453 <br>
	- Multimedia Tools and Applications (MTAP): 2.577 <br>
	- Journal of Electronic Imaging (JEI): 0.829<br>
	
	
			
			
	</div>			
		</div>	
	</div>
	
		<script type="text/javascript" src="./files/jquery.min.js.下载"></script>
		<script type="text/javascript" src="./files/bootstrap.js.下载"></script>
		<script type="text/javascript" src="./files/jquery.banner.js.下载"></script>
		<script type="text/javascript" src="./files/jquery.prettyPhoto.js.下载"></script>		
		<script type="text/javascript" src="./files/jquery.isotope.js.下载"></script>	
		<script type="text/javascript" src="./files/main.js.下载"></script>				
	


</body></html>
